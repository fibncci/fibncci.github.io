---
layout: post
title: "spark查询"
date: 2021-10-08
tag: 机器学习machineLearning	
---

`[TOC]`



# spark查询



## 1.1 SQLContext
SQLContext是通往SparkSQL的入口。

```sql
one = sqlCtx.sql("sql语句")

scala> spark.sql("select * from ptable").show(100,false)
+---+---+----+
|c1 |c2 |step|
+---+---+----+
|1  |2  |a   |
|3  |4  |a   |
|5  |6  |b   |
|7  |8  |b   |
+---+---+----+

 
```

## 1.2 HiveContext
HiveContext是通往hive入口。**HiveContext具有SQLContext的所有功能。 

```sql
one = hive.sql("sql语句")

```



## 1.3 SparkSession
SparkSession是在Spark 2.0中引入的，通过访问SparkSession，我们可以自动访问SparkContext。

```spark2.0
```



## 1.4 操作

高阶操作

```sql
三个join一起上：
one = s_loy_number.join(cx_mem_auto,col('mem_id') == col('member_id'),'left').join(其他表，关联).join()  #可以直接写 


LEAD()函数访问当前行之后的特定物理偏移量的行
LEAD(net_sales,1) OVER ( PARTITION BY brand_name ORDER BY month ) as next_month_sales


.write.saveAsTable("dh_seiz.BMD_yyn_vin_mile2a" ,'parquet','overwrite')  写文档 parquet拼花地板；overwrite覆盖
data.write.saveAsTable(‘shuangshi_dh.dot_analysis’, mode=“append”, format=“hive”, partitionBy=“dt”)

mode=’overwrite’ 模式时，会创建新的表，若表名已存在则会被删除，整个表被重写。
而 mode=’append’ 模式会在直接在原有数据增加新数据。
partitionBy指定分区字段，默认存储为 parquet 文件格式 
partitionBy=['pt_day']；partitionBy='parquet'

df.repartition(5).write.saveAsTable(...)
或
df.repartition(5).registerTempTable('temp_table')
以上设置一个分区只会保存 5 个数据文件。
```

### 1.5 函数

```sql

win = 
st_bo = sqlCtx.table("库.表1").             --库表
join(表2,表1.VIN==表2.B_VIN,'left').drop('B_VIN') -- 左链接，删除B_VIN
selectExpr("vin as B_VIN","列名").          --查看列名
withColumn("rn", row_number().over(win)).  --创建的窗口函数
filter("rn = 1").   												where条件
drop('rn')   																删除字段

sqlCtx.table("dh_seiz.BMD_yyn_vin_mile4").   # library 库；表 table
filter("if_ext=1").      # 过滤
selectExpr("vin","'里程' as EXT_INS_EXP_desc").  # 选择 表达式 expression
distinct()       # 不同的
orderBy(desc('b'))
groupBy("")

time time() 返回当前时间的时间戳（1970纪元后经过的浮点秒数）

spark dataframe的select和selectexpr的区别

select&selectExpr区别
select是把要遍历的集合ienumerable逐一遍历，每次返回一个t，合并之后直接返回一个ienumerable，而selectmany则把原有的集合ienumerable每个元素遍历一遍，每次返回一个ienumerable，把这些ienumerable的“t”合并之后整体返回一个ienumerable。
 
selectExpr：可以对指定字段进行特殊处理，可以直接对指定字段调用UDF函数，或者指定别名等。传入String类型参数，得到DataFrame对象。if \case  when \ contant
　　示例，查询id字段，c3字段取别名time，c4字段四舍五入：
jdbcDF .selectExpr(“id” , “c3 as time” , “round(c4)” ).show(false) 

cast(celling(rand()*8))  取值1-8
```



### 1.6 sql语句

正常的 sql语句语法

```SQL
SELECT    case when 条件 then 结果 else 结果2  end  
FROM      表  
join （left join \ right \ inner (两者都存在)\full join（取并集） ）
WHERE  	   过滤条件
GROUP BY   分组条件
ORDER BY  (DESC)  排序依据，正序、倒序
LIMIT 100    显示多少行

union all 使用 " select *  from new   union all select *  from old "，  操作只要保证数据 列名，列的类型相同，列的顺序可以不同，根据第一列的顺序来排序；针对 列的类型不同，列名相同的时候， 需要 " select  字段名1  from new   union all select *  from old " ，此时字段一的类型在new中是string，而在old中为 decimal(38,0) 。 写法必须是 “字段1” 在new中。

补集
select B.*
from test.dbo.m2 B
where NOT EXISTs(select 1 from test.dbo.m1 A where A.姓名=B.姓名 and A.年龄 = B.年龄 and A.学历=B.学历)
```





# 业务

### DTC实时消息发送场景

```
DTC实时消息发送场景
flink
KafKa
Flume
流式数据处理 ：针对 爆胎、遇险、车上故障损坏报警，传送到云端onstar ；然后我们针对后台的数据，进行实时操作，进行风险分类划分等级，作出相应，传送给安吉星。
一个imoment项目的子分类
```





### 保养到期提醒别克官微

````
消息模版数量3  日均发送数量 8383
消息模版数量1  日均发送数量 1
````



## imoment

- 场景
- 1、消息模版数目。
- 2、日均发送数量。
- 3、日均到达数量  
- 4、日均点击数量
- 5、日均点击率 =
- 





| 消息模版数目 | 日均发送数量 | 日均到达数量 | 日均点击数量 | 日均点击率4/3 |
| ------------ | ------------ | ------------ | ------------ | ------------- |
| 27--10-11    | 78244        | 3377         |              |               |
| 30--10-10    |              |              |              |               |
| 30--10-9     | 94916        | 5218         | 986          | 18.90%        |
| 27--10-8     | 78277        | 3845         | 708          | 18.41%        |
| 25--10-7     | 63182        | 569          | 124          | 21.79%        |

具体



| 消息模版数目-2020-09-30举例                      | 日均发送数量 | 日均到达数量 | 日均点击数量 |        |
| ------------------------------------------------ | ------------ | ------------ | ------------ | ------ |
| 1. 凯迪app金卡保级礼提醒（提醒新）               | 20           | 0            | 0            |        |
| 2. 凯迪APP入会关怀纪念礼（提醒新）               | 3153         |              |              |        |
| 3.凯迪官微金卡保级礼（提醒新）                   | 15           | 无数据       |              |        |
| 4.凯迪官微入会关怀纪念礼（提醒新）               | 1781         |              |              |        |
| 5.雪佛兰**保险**到期提醒（车机品牌APP）          | 674          | 无数据       |              |        |
| 6. 雪佛兰保养到期提醒（车机品牌APP）             | 465          |              |              |        |
| 8.雪佛兰年检到期提醒（车机品牌APP）              | 342          | 无数据       |              |        |
| 9.别克保险**到期提醒（车机品牌APP）              | 1386         |              |              |        |
| 10. 别克保养到期提醒（车机品牌APP）              | 1514         |              |              |        |
| 11.别克年检到期提醒（车机品牌APP）               | 295          |              |              |        |
| 12.凯迪拉克保险到期提醒（车机品牌APP）           | 1455         |              |              |        |
| 13. 凯迪拉克保养到期提醒（车机品牌APP）          | 1418         |              |              |        |
| 14.凯迪拉克年检到期提醒（车机品牌APP）           | 563          |              |              |        |
| 15. 加油CP合作车机端--------3条                  | 18938        |              |              |        |
| 16.大雾天气预警-------3条                        | 10539        |              |              |        |
| 17.凯迪官微**保养到期提醒** && <u>别克可能有</u> | 593          | 437          | 84           | 19.22% |
| 18.雪佛兰官微**保养提醒**                        | 600          | 2            |              |        |
| 19.雪佛兰官方APP保养提醒                         | 820          |              |              |        |
| 20.DTC实时消息发送场景-----3条                   | 33507        |              |              |        |
| 合计                                             | 78078        | 439          | 84           | 19.13  |
| 1-别克官微当日积分过期提醒                       | 215          | 108          | 21           | 19.44  |
| 2-雪佛兰官微保养提醒                             | 1131         | 11           |              |        |
| 12日出错                                         |              |              |              |        |
|                                                  |              |              |              |        |
|                                                  |              |              |              |        |
|                                                  |              |              |              |        |
|                                                  |              |              |              |        |
|                                                  |              |              |              |        |
|                                                  |              |              |              |        |



当月的20号没有，当月的数据



MySQL `TIMESTAMP`是一种保存[日期](http://www.yiibai.com/mysql/date.html)和[时间](http://www.yiibai.com/mysql/time.html)组合的时间[数据类型](http://www.yiibai.com/mysql/data-types.html)。 `TIMESTAMP`列的格式为`YYYY-MM-DD HH:MM:SS`，固定为`19`个字符。 

pandas使用NumPy的数组和dtypes作为序列和数据框中列的数据类型，NumPy支持的数据类型是float、int、bool、timedelta64[ns]。pandas扩展了NumPy的类型系统，用dtype属性来显示元素的数据类型，pandas主要有以下几种dtype：

- 字符串类型：object
- 整数类型：Int64，Int32，Int16, Int8 
- 无符号整数：UInt64，UInt32，UInt16, UInt8 
- 浮点数类型：float64，float32
- 日期和时间类型：datetime64[ns]、datetime64[ns, tz]、timedelta[ns]
- 布尔类型：bool



 

要修改Spark DataFrame的列类型，可以使用"withColumn()"、"cast转换函数"、"selectExpr()"以及SQL表达式。需要注意的是，要转换的类型必须是DataType类的子类。

在Spark中，我们可以将DataFrame列修改（或转换）为以下类型，它们都是DataType类的子类：

- ArrayType
- BinaryType
- BooleanType
- CalendarIntervalType
- DateType
- HiveStringType
- MapType
- NullType
- NumericType
- ObjectType
- StringType
- StructType
- TimestampType

停业基盘维系

2021-03-01 





阳性 =1 要发leads，

阳性 ： 需要进行回站后才去

、阴性 



二分类模型





## icare

- 11.30开始运行





## 算法

逻辑程序员：只会写业务，不会涉及到框架、算法、也不会涉及到底层；不要做这种；快速学习python3-10、加班现状要更好的身体

1. 粒子算法
2. 距离算法：聚类算法













 
