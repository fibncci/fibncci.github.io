---
layout: post
title: "sql-3周笔记"
date: 2019-12-10
tag: sas
---





# 第一周

1.脑图 xmind 

2.markdown总结

3.代码三遍(除了视频和html后缀的文件)



window查看ip ipconfig

1.连接数据库

 在命令行输入 mysql -u 用户名 -p 密码 -h 127.0.0.1 -P3306



# 1.对库的操作：



0。查看数据库 show databases;   查看所有的数据库

1. 建数据库create database **库名** （字段名 类型 【约束】，
   字段名 类型 【约束】）default charset=utf8
   2.打开数据库（切换数据库/进入数据库） use 库名
   3.查看当前在哪个数据库  select database();
2. 删除数据库 drop database 库名

<u>对2019-11 ubuntu 的设置</u>

```sql

mysql> create database fib;
Query OK, 1 row affected (0.00 sec)

mysql> use fib;
Database changed
mysql>  select database();
+------------+
| database() |
+------------+
| fib        |
+------------+
1 row in set (0.00 sec)
```



# 2.对表操作

1.查看当前库中的所有表·show tables;
2.查看表结构 desc 表名;
3.查看当前表中的所有数数据 select * from 表名;

4. 删除表：drop table 表名;

## 2.1修改字段

一.  modify 只能修改字段类型(change\modify    字段类型 字段名)

1.修改 userinfo表 的aa字段类型为字符串： alter table userinfo modify aa varchar(10);
2.userinfo表bb字段已经有了数值类型的值 将他的类型改为字符串类型：  alter table userinfo modify bb varchar(10);

3. aa字段为字符串类型 并且有值为‘88’  将他转成int类型：  alter table userinfo modify aa int(10);
   4.bb字段为字符串类型 并且有值为‘玲玲’ 将他转成int类型 会报错： alter table userinfo modify bb int(10);
   注意：（1）将数值类型转成字符串类型没问题
      （2）将字符串类型转成数值类型，要考虑数据中有没有非数值类型的内容 如果有不能修改
       （3）如果都是数值类型的字符串 可以转

二.  change 不仅能修改字段类型还能修改字段名
1.将bb字段名修改成len:	alter table userinfo change bb len char(30);
2.在修改字段时 必须要写字段类型 不写会报错:  alter table userinfo change len bb;  报错
3.使用change 只修改字段类型 不修改字段名: alter table userinfo change aa aa char(30);
注意：使用chage 修改字段信息的时候，不修改字段名 字段名要写两遍

## 2.2 删除字段

1. 删除aa字段 alter table userinfo drop aa;



## 2.3 修改表名：将表userinfo的表名修改成 users:	 alter table userinfo rename as users;

1.修改自增值：alter table 表名 auto_increment = 数字只能向后改 不能向当前自增值前面改

## 2.4表引擎：

1. innodb 支持事务
   事务：将一组操作当做一个执行单元，这一组就是一个事务
    		 这一组sql只有一个执行失败，全部撤销
      要么都成功要么都失败
   特点：将数据存到一个文件 字段信息 索引信息 数据
   效率问题; innodb 的效率低于myisam
2. myisam:不支持事务,数据不安全, 将数据存在三个文件
   innodb 重启服务之后自增至会重新计算
   myisam 重启之后自增至不会重新计算

# 3.数据操作

1.添加数据 insert into 表名(字段1，字段3，字段2) values(值1，值3，值2);
1.1一次只添加一条数据
1.1.1)指定所有字段添加： insert into 表名(有几个字段写几个字段) values(写了几个字段给几个值)
1.1.2)指定部分字符段添加： insert into 表名（部分字段） values(部分值)
1.1.3)不指定字段添加：insert into 表名 values(所有的值)

1.2一次添加多条数据
1.2.1指定所有字段添加:insert into 表名(所有字段) values(所有的值),(所有的值)
​    指定部分字符段添加
​    不指定字段添加
2.修改（更新）数据 update 表名 set 字段名 = 新值 where 条件;
​	注意：如果不添加where 会将表中的所有数据的指定字段更新
2.1不添加where条件  所有数据都更新
更新id为2,3,4的数据邮箱:update users set email = 'new@qq.com' where id = 2 or id = 3 or id = 4;
或者：update users set email = 'new2@qq.com' where id in (2,3,4);
2.2 更新 name='波波' 并且 len = '123' 的数据，将phone更改为138： update users set phone = '138' where name='波波' and len='123';
2.3更新id的值大于3的数据，将phone 改成10086： update users set phone = '10086' where id > 3;
3.查询数据  select * from 表名 where 条件;
1.查询所有字段信息:  select * from 表名 where 条件
2.只查询指定字段的数据  id 和 name:   select id,name from users;
3.where 条件查询 :
3.1 查询py1的所有人员信息: select * from stu where class='py1';
3.2 查询py1 sex为0的 人员信息: select * from stu where class = 'py1' and sex = 0;
3.3 查询py1 或者py2 中的 sex=1 的数据: select * from stu where (class = 'py1' or class='py2') and sex=1;
3.4 查询年龄在 18-25 之间的所有人员信息 ：select * from stu where age between 18 and 25;
3.5查询年龄在 18-25 之间的py2所有人员信息 ：select * from stu where age between 18 and 25 and class='py2';

```
查询年龄分别为 18 28 30 的人员信息（自己写，两种方式）
	where age =18 or age = 28 or age = 30 
```

4.删除数据 delete from 表名 where 条件;
  注意： 一定要加where 不加会将表中的所有数据删除
4.1删除len=12的数据: delete from users where len='12';
4.2-- 删除表中id值为4到10的数据  不包括4 和10:::::  delete from users where id > 4 and id < 10;

5.like子句查询 也叫模糊查询

```
%: 代表任意位的任意字符
_：一位任意字符
在实际开发中不建议将%，_ 写到第一位，写到第一位会造成索引使用不上

查询 姓名中带龙的 人员信息
select * from stu where  name like '%龙%';

查询 姓名以郭开头的 人员信息
select * from stu where name like '郭%';

查询名字是两个字 人员信息
select * from stu where name like '__';

转移字符：

    ^: 带表以指定字符开头
    $: 以指定字符结尾
```

```
姓郭的人员信息
select * from stu where name regexp '^郭';
姓名中以数字4结尾的 人员信息
select * from stu where name regexp '4$';
```

6.聚合函数：
​    最大值 最小值 求和 平均值 统计

安装 pymysql   ：输入pip install pymysql



# 4.数据类型

1.数值类型
​      tinyint  小数值类型 1字节
​      int      4字节
​      decimal  以字符串形式表示小数
2.时间日期类型  datatime   混合日期时间  年/月/日 时:分:秒

3. 字符串 char 定长字符串
   password char(32)   当前字段最多存储32个字符，就算只存储了1个字符也按32个字符的宽度去计算
       123
       varchar 变长字符串  
       varchar(255) 当前字段最多存储255个字符 如果存了1个字符，就按一个字符大小计算
       123

# 5.索引： 

默认建表时必须要有一个主见索引，一张表中只能有一个主键
​        主见索引 primary key   不能为空且唯一（不能重复 ；不能为空；具有唯一性）
​        唯一索引 unique        具有唯一性，不能重复 但是可以为空
​        普通索引 index可以重复  ;可以为空
​        全文索引 不支持中文

1.添加索引
​            给name添加唯一索引
​            alter table users add unique u_name(name);
​            给phone添加唯一索引 唯一索具有唯一性 如果有重复的数据 就添加不上了
​            alter table users add unique u_phone(phone);

```
        给email 添加普通索引 普通可以为空 可以重复
        alter table users add index i_email(email);
```

2.删除索引
​            alter table users drop index 索引名
​             删除索引时 不管是唯一索引 还是普通索引 都是用index

```
    删除主建索引：（了解测试）
        先删除主键索引的自增属性
        在删除主键索引
```







课件导航无法跳转解决办法
https://www.jianshu.com/p/68270f033124

1.聚合函数
​	max() : 最大值
​	min() ：最小值
​	count() ：一共有多少条数据
​	sum()   ： 求和
​	avg()   ： 求平均值

```
统计 当前表中一共有多少条数据
 select count(*) from users;

统计班级中 最大年龄 和最小年，平均年龄，年龄总和
select max(age),min(age),avg(age),sum(age) from users;
# 起别名
select max(age) max_age,min(age) min_age,avg(age) avg_age,sum(age) sum_age from users;
```



2.group by 分组 和having
​    格式： select * from 表名 group by 字段名
​    按照某一个字段进行分组
​    having 对分组后的数据进行删选 类似与where

```
统计当前表中都有哪些班级

统计每个班级的人数
 select class,count(*) from stu group by class;
统计男生和女生个多少人
 select sex,count(*) from stu group by sex;

 select sex,count(*) from stu group by sex;

统计每个班级男生有多少人
select class,sex,count(*) from stu where sex = '1'  group by class;

统计班级人数大于3的班级
select class,count(*) num from stu group by class having num > 3;
```

3.order by 排序 asc desc
​    可以根据指定字段进行排序操作
​    默认是升序排序asc 所以如果是升序排序可以省略asc

```
按照年龄进行升序排序
select * from stu order by age asc;
按照年龄进行降序排序
select * from stu order by age desc;
多个字段排序：对班级进行升序排序 在将每个班级的学员按照年龄进行降序排序

 第二个字段的排序是在 第一个字段排序的基础之上进行的
select * from stu order by class,age desc;
```

4.limit 分页
   limit num1,num2;
   num1 从哪一条数据之后取
   num2 取多少数据
​	
	查询第一条数据
	select * from stu limit 1;
	

```
查询前2条数据
select * from stu limit 2;

跳过前2条数据 在查询2条数据
select * from stu limit 2,2;

跳过前4条  在查询两条数据
select * from stu limit 4,2;

分页： 按照每页两条数据 取第一页
       select * from stu limit 0,2;   ---->1  页码-1
       取第二页
       select * from stu limit 2,2;   ---->2  （页码-1） * 每一页的条数
       select * from stu limit 4,2;   ---->3   （3-1）* 2
       select * from stu limit 6,2;   ---->4   （4-1）* 2

```

练习：
​	获取年龄最大的3位同学的信息
​	select * from stu order by age desc limit 3;

```
获取每个班级的平均年龄，并按照平均年龄降序排序
select class,avg(age) avg_age from stu group by class order by avg_age desc;

获取人数最多的班级
select class,count(*) from stu group by class order by count(*) desc limit 1;

```

注意条件的顺序：
​    where group by--》having order by limit

```
where 条件只能写在最前面 不能写在以上任意一个 条件后面
limit: 写在最后

```







-- 记住流程就可以了
5.数据的导入和导出
​    退出mysql客户端
​    导出库
​    mysqldump -u 用户名 -p123456 库名 > 本地的存储路径.sql

```
导出库表
mysqldump -u 用户名 -p123456 库名 表名 > 本地的存储路径.sql

```

导入：
​    导入库的时候如果编码有问题 会报一下错误
​    ERROR at line 92: Unknown command '\''.
​    mysql -u root --default-character-set=utf8 库名<文件.sql
​    导表：
​        mysql -uroot -p1234567 库名< sql文件

6.授权操作
​	
	格式：grant 允许操作 on 库名.表名 to '账号'@'来源' identified by '密码';
	给zhansgan用户设置对所有库和所有表的 所有权限
	grant all on *.* to 'zhansgan'@'%' identified by '123456';
	给lisi用户对py19库 设置所有权限
	grant all on py19.* to 'lisi'@'%' identified by '123456';
	给wangwu设置对py19库只有查询权限
	grant select on py19.* to 'wangwu'@'%' identified by '123456';

7.windows如果忘记mysql管理员密码如何操作(了解流程即可)
​	
​	
	1.找到mysql配置文件 C:\ProgramData\MySQL\MySQL Server 5.7
	2.在[mysqld] 下面添加 skip-grant-tables 保存
	3.重启mysql服务
	4.打开命令行 mysql -uroot -p  不用输入密码
	5.use mysql 切换到权限库
	6.执行修改密码 Update user set authentication_string=password('1234567') where user='root';
	7:执行刷新权限 flush privileges;
	

```
修改成功后：要把配置文件改回来 然后在重启服务

使用新密码链接数据库
```

-- 需要掌握的
8.python 链接数据库

```
pymysql的安装

    python的包管理工具 pip  帮助我们下载三方包 解决依赖问题

    pip install 包名
    pip list  查看当前环境里的所有包
    指定版本安装 如果不指定版本 安装最新版本
    pip install pymysql==主版本号.辅版本号

fetchone() 返回单条数据
fetchall() 返回多条数据
rowcount   获取影响的条数

ORM(对象关系映射)
```

# 前端

```
网页开发，
HTML: 页面结构                               结构层
css ： 页面的样式                             表现层
Javascript(js)：负责页面的动态效果和交互        行为层

html: 超文本标记语言   主要用来制作网页

制作一个网页 需要的基本结构是什么
1.html是有标签组成 一般标签都是成对出现 <标签名>内容</标签名>

img标签 src 图片地址
a 标签 href 连接地址
 <!--音频标签 src 音频地址-->
<audio src="./贝多芬 - 致爱丽丝 - 钢琴版纯音乐.mp3" controls="controls"></audio>
<!--视频标签 src 视频地址 -->
<video src="./课时1：求函数的定义域免费.mp4" controls="controls"></video>

form action 数据的提交地址
     method: post  网络请求方式   不会将提交的数据 明文显示
             get   网络请求方式   会将提交的数据拼接到url地址后面
```





#  第二周

# day4——ip地址连接

1.html节点树

​    只要是通过标签声明内容都是节点



​    html文档 文档节点

​    标签节点

​    文本节点

​    属性节点

​    注释节点

​    

    <a href="http://www.baidu.com">点击我跳转到百度</a>

    <div>

           <a href="http://www.baidu.com">点击我跳转到百度</a>

​    </div>







2.节点树中节点的关系

​    节点树中的节点，主要来表示标签的层级

​    由 子节点 父节点 兄弟节点

​    除了跟标签 每一个标签都有一个父元素节点

​    一个标签都会有多个兄弟元素节点，或者有多个子元素节点

​	



3.css定义及语法格式

​    css:主要负责页面元素的显示样式  重叠样式表

​    语法格式：

​        选择器{key:val;key:val}



​     选择器：就是让html和css连接在一个的 方法的名字





​	

4.如何在页面中使用css



​    a.外链式

​        通过link标签的href导入外部的css文件

​    b.嵌入式

​        通过style标签 将css代码嵌入到html文档中

​    c.内联式 行间样式

​        通过元素的style属性来讲css写在标签的行间





​	

​	

5.css基本的选择器

​    a.id选择器

​        通过标签id属性的值来获取要设置样式的元素

​        选择符 #

​        id具有唯一性：一个id的值只能在当前html中出现一次

​    b.标签选择器

​        通过标签名来获取元素

​        注意：页面当中所有选择器选择到的标签都会受到影响

​        标签名{}



​    c.类选择器

​        通过元素的class属性的值 来获取元素

​        选择符 .

​        类选择器 可以重复使用 ，一个标签可以设置多个class值

​    

​     e.后代选择器 层级选择器

​        只要符合选择器条件 不管是子元素，还是孙子元素 都会受到影响

​        选择符 选择器之间用 空格隔开

​     f.子选择器

​        只找符合选择器条件的下一级元素（儿子元素） 不会去找下下一级

​        选择 >

​     g.属性选择器

​        根据标签的属性来获取元素

​        格式 标签名[属性名=属性值]{}









6.伪类及伪元素选择器

​    hover

​    after

​    before



​    :first-child{}

​    :last-child{}

​    :nth-child(个数){}





​	



网络



1.什么是网络



​	定义：辅助设备连接的工具

​	使用网络的目的：数据的共享



2.网络协议的诞生TCP/IP协议族

​	tcp/ip 是一套互联网的标准协议  tcp/ip协议族

​	网络模型：

​	    实际用四层模型：

​	        网络接口层        物理层

​	                        数据链路层

​	        网际层

​	        传输层

​	        应用层           会话层

​	                        表示层

​	                        应用层

​	







3.ip地址

​    地址：用来标记一个位置

​	ip地址干嘛用的：一个ip对应互联网当中唯一一台联网的设备

​	分类：ipv4

​	       A B C D E



4.端口号



​	端口号干嘛用的：标识网络通讯的应用程序 一个应用程序会有一个独立的端口号

​	端口号的范围：0-65535

​	端口号的分类：

​	    知名端口号：80端口 web服务端口 22 ssh端口

​	        范围：0-1024

​	    动态端口号：

​	        1025-65535

​	        建议使用3000以上端口号





5.udp协议，TCP协议

​    udp,tcp属于传输层协议

​	

​	udp特点：负责发送数据，不关心数据是否送达， 对于网络环境依赖较大，传输速度快，传输数据有大小限制64k，数据传输不安全，无序

​	        用在聊天软件

​	tcp特点：

​	    传输数据前 要保证双方都能连接成功 传输速度慢 数据传输大小没有明确限制，数据传输安全，有序

​	

​	三次握手：

​	建立连接 三次握手

​	

​	ACK：确认标志

​	SYN：同步标志

​	第一次握手：主机A发送位码为syn＝1,随机产生seq number=1234567的数据包到服务器，主机B由SYN=1知道，A要求建立联机；

​	

​	第二次握手：主机B收到请求后要确认联机信息，向A发送ack number=(主机A的seq+1),syn=1,ack=1,随机产生seq=7654321的包

​	

​	第三次握手：主机A收到后检查ack number是否正确，即第一次发送的seq number+1,以及位码ack是否为1，若正确，主机A会再发送ack number=(主机B的seq+1),ack=1，主机B收到后确认seq值与ack=1则连接建立成功。

​	

​	完成三次握手，主机A与主机B开始传送数据。





​			





6.python实现udp数据传输



​	服务端：--s  是云

​	import socket

​	# 创建套接字

​	udp_s = socket.socked(socket.AF_INET,socket.SOCK_DGRAM)

​	# 绑定ip地址

​	udp_s.bind(('',8080))

​	# 接受消息

​	data,addr = udp_s.recvfrom(1024)

​	print(data)

​	

​	# 关闭套接字

​	udp_s.close()





​		

​	客户端：--c 是本机

​	import socekd

​	# 创建套接字对象

​	udp_c = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)

​	# 发送消息

​	udp_c.sendto('hello'.encode('utf-8'),('192.168.1.15',8080))

​	# 关闭套接字

​	udp_c.close()







7.python实现tcp数据的通讯



\```python

服务端：

1.创建套接字对象

2.绑定ip地址和端口号

3.监听

4.接受消息

5.返回消息

6.关闭套接字



import socket

tcp_s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)

tcp_s.bind(('',8080))

tcp_s.listen()

s,addr = tcp_s.accept()

data = s.recv(1024)

print(data.decode('utf-8'))

s.send('hello'.encode('utf-8))

s.close()

tcp_s.close()



客户端：

1.创建套接字对象

2.创建连接

3.发送消息

4.关闭套接字对象



import socket

tcp_c = socket.socket(socket.AF_INET,socket.SOCK_STRAM)

tcp_c.connect(('192.168.1.15',8080))

tcp_c.send('hello'.encode('utf-8'))

tcp_c.close()

\```







​	



8.HTTP协议





​	超文本传输协议：将一台设备上的 超文本（html）按照url的指示，传输给另一台设备 的协议

​	    属于应用层协议 基于传输层tcp协议

​	超文本：html

​	

​	URL(统一资源定位符)：为了标识互联网中的唯一资源（视频，音频，图片，文档）

​	

​	一个完整的url地址包括哪几部分：

​		http://www.aspxfans.com:8080/news/index.asp?boardID=5&ID=24618&page=1#name

​		1.协议部分： //之前 http:

​		2.域名部分：//之后 到第一个/之前或者:之前

​		3.端口部分：web默认端口80 可以省略不写 :之后 到/或者& 或者是# 或者到最后

​		4.虚拟路径：第一个/到最后以/之间是虚拟路径部分

​		5.参数部分：？之后 到最后或者是# 之间, 一个url可能会有多个参数, 

​							参数和参数之间使用&分割.

​		6.锚点： # 之后 到最后是锚点



9.HTTP特点：

​	



​	当用户在浏览器地址栏输入地址 到看到响应内容，中间都发生了什么事情





​	http链接方式：

​	    短连接： 没发送一次请求，完成后立马断开连接

​	    长连接：一直保持连接

​	            但是可以设置连接的时间

​	            一次请求 设置连接时间为3分钟

​	            3分钟内有新的请求，就在最后一次请求的基础上在重新计时

​	            如果超过三分钟没有新的请求，自动断开连接





​	无状态性：

​	    htpp 是不能记录用户的上一次操作行为的

​	    如果需要记录用户信息 就需要设置

​	        cookie ：将信息存储到客户端本地 一般不建议存用户信息 容易泄露信息

​	        session ：将信息存储到服务端 有效的保证了信息的安全

​	                创建一个session 会生成一个sessionid 将sessionid存到cookie中

​	                session一般配合cookie去使用 用于验证用



10.http报文结构：



​	请求报文：

​	即从客户端(浏览器)向Web服务器发送的请求报文。报文的所有字段都是ASCII码。







​	响应报文：

​	状态码(Status-Code)是响应报文状态行中包含的一个3位数字，指明特定的请求是否被满足，如果没有满足，原因是什么。状态码分为以下五类：

​	1xx：指示信息--表示请求已接收，继续处理。

​	2xx：成功--表示请求已被成功接收、理解、接受。

​	3xx：重定向--要完成请求必须进行更进一步的操作。

​	4xx：客户端错误--请求有语法错误或请求无法实现。

​	5xx：服务器端错误--服务器未能实现合法的请求。





​	请求头中常见的请求方法：

​		get:

​		post:



11.响应状态码

​	

常见的请求报文和响应报文



\13. http代理



​    \```py

​    

​    HTTP代理又称Web缓存或代理服务器(Proxy Server),是一种网络实体， 能代表浏览器发出HTTP请求，并将最近的一些请求和响应暂存在本地磁盘中， 当请求的Web页面先前暂存过，则直接将暂存的页面发给客户端(浏览器)，无须再次访问Internet。

​    \```



​    



\14. 爬虫简介





# day5-0409整理

1.http报文结构：

​    掌握理解 服务端如何去解析 你的报文 以及报文的结构

​	请求报文：

​		请求行

​		首部行

​		请求体

​		    如果发送的是get请求 请求体是空的

​		    只有发送post请求时 请求体当中是 要发送的数据



​	响应报文：

​	    状态行 协议版本 状态码 描述短语

​	    首部行

​	    响应体

​	        服务端响应的内容 也是用户需要的内容

​		

​	请求头中常见的请求方法：

​		get: 会将数据拼接到url地址后面 有数据的大小限制 受浏览器限制

​		post:

​		    不会讲数据明文显示在url地址栏 而是将数据放到 请求体 理论上没有数据大小限制





​	

​	

2.常见的请求报文和响应报文

​    user-agent:浏览器标识 记录用户使用的浏览器版本

​    cookie: 用户认证信息

​    Referer： 防盗链 记录你的访问时从哪个连接跳转来的 类似于介绍人



常见的状态码

​	

\3. http代理

  缓存服务器 当用户发送请求时 先访问代理服务器，代理服务器会检测当前请求是否有缓存，如果有缓存，直接将缓存信息返回给用户

  如果没有缓存信息，就代替用户向远程服务器发送新的请求，请求回来后进行缓存 并把响应回来的数据返回给用户



\4. 爬虫简介



  数据从哪来

​      企业自己用产生的用户数据：百度指数 新浪微博指数 腾讯浏览指数

​      数据平购买数据：贵阳大数据交易所

​      政府的公开数据：国家数据

​      数据咨询管理公司：



  什么是爬虫

​      通过脚本程序 获取网络数据

  	



  如何获取网络数据 b/s

​      url:

​      http/https

​      html

​          json

​          xml

​          文本

  	



  构建爬虫的主要步骤

​      1.确定需求

​      2.确定url

​      3.分析网站

​      4.发送请求

​      5.解析响应的数据

​          返回时html

​          获取导数据 进行存储

​          如果有新的url 继续循环执行4步

​      5.存储数据

  	



  为什么用python做爬虫

​      php 是世界上最好的语言 但是他不是专门干爬虫 对多进程的支持较弱

​      java 社区完善 是python爬虫的最大竞争对手 代码量过大 维护困难

​      c++

​      python 语法简单 代码量少 易于维护 大量包和模块



\5. 爬虫中的反反爬虫

​    1.headers

​        user-agent

​        Referer

​        cookie

​    2.添加代理



​    3.js加密

​        逆向解析

​    4.登录

​        模拟浏览器的http post请求

​        直接获取登录后cookie

​    5.验证码

​        通过第三方的打码平台

​        机器学习 自己训练模型





6.虚拟环境的使用

​    一台电脑配置多个python环境 隔离三方包的污染

​    安装：

​        pip install virtualenv

​    创建虚拟环境

​        virtualenv v1

​    激活虚拟环境

​        切换到 v1下的Scripts目录下

​        执行 activate

​        如果命令行前面显示 （v1） 代表我们进入了虚拟环境

​    退出虚拟环境

​        执行deactive



​    pip list

​    pip show 包名

​    

​    pip freeze 查看当前环境所有三方包

​    pip freeze > 导出的文件名.txt

​    

​    pip install -r 导出的文件名.txt   将导出的三方模块批量安装到新的环境



7.urllib模块的使用



​	基本使用案例 百度首页

​	构造请求头案例 西祠代理 http://www.xicidaili.com/

​	百度搜索 url中文转码

​	

​	作业：

​	百度贴吧 爬取多页数据

​		用户输入贴吧名

​	

​		输入下载页码范围







urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1051)>



https://www.baidu.com/s?wd=帅哥



\# **拓展做完减一遍作业**

https://tieba.baidu.com/f?kw=%E7%BE%8E%E5%A5%B3          1

https://tieba.baidu.com/f?kw=%E7%BE%8E%E5%A5%B3&pn=50    2

https://tieba.baidu.com/f?kw=%E7%BE%8E%E5%A5%B3&pn=100   3





# day06

1.ajax 异步加载数据
    主要负责数据的交互
    异步加载 在页面不刷新的情况下 更新页面的局部内容
作业：https://movie.douban.com/chart  其他分类的电影信息 存储到本地json文件

2.登录
    a.先从浏览器登录 获取cookie信息 在脚本程序中发送请求 最直接 最暴力 最方便 需要长期维护
    b.模拟表单提交

Charles mac 抓包工具

豆瓣url地址分析：
    https://movie.douban.com/j/chart/top_list?type=17&interval_id=100%3A90&action=&start=20&limit=50
    https://movie.douban.com/j/chart/top_list?type=17&interval_id=100%3A90&action=&start=40&limit=20
    https://movie.douban.com/j/chart/top_list?type=17&interval_id=100%3A90&action=&start=60&limit=20

```
感觉进度条一下拉不到底有可能就是异步加载 不一定全部都是
有异步请求必定是json文件吗???? 不一定 80%都是json格式 有可能是 其他字符串 html代码字符串
有异步请求 怎么看出来是json格式的数据   讲慢一点  查了、看异步加载的返回结构 如果是【{}，{}】{} 就是json格式
```





# day07

解析字符串 提取数据信息



 re 模块



import  re



 python 实现正则匹常用的方法



re.compile() 编译一个规则对象



re.search() 从字符串开头检索到结尾 返回第一次匹配到的内容



re.match() 从头检索到尾部 默认在规则前面添加^



re.findall() 从头检索到尾部 返回所有符合规则的内容 返回一个列表



re.finditer() 从头检测到位 返回一个迭代器







 什么是正则？



检索匹配 符合指定规则的文本



 正则表达式 由普通字符和元字符组成



\1. 普通字符



 2、转义字符



​     \d    单个的数字



​     \D    单个非 \d



​     \w    单个的 数字字母下划线



​     \W    单个非 \w



​     \s    单个空白符



​     \S    单个非 \s







 3.特殊的元字符



​     . 除了换行符以外的 任意一个字符



​     \* 匹配0或多次



​     \+ 匹配1 或多次



​     ？ 禁止贪婪



​     () 子组 隔离 分组  不会影响整个规则



​     |  或  多个匹配规则



​     [] 一个字符集的范围



​     {} 限制匹配次数 {3} 匹配三次 {3，} 至少三次  {3,6} 最少三次最多6次



​     ^



​     $



















 作业：



​     用户输入 贴吧名



​     开始页



​     结束页







​     获取当前帖子列表页的所有a连接 href



​     帖子详情：抓取地址



​     再去下载当前详情页的图片







 拓展 爬取 https://www.xicidaili.com/nn/



 提取 ip和端口号 存到本地的json文件



 {'http':ip,'port':port}











 反爬 js逆向工程



\```python

salt : "" +  str(int(time.time()*1000)) +  str(random.randint(0,10))

n.md5("fanyideskweb" + 'python' + salt + "@6f#X3=cCuncYssPsuRUE")

import hashlib

def get_token():



​    md5str = "fanyideskweb" + 'python' + salt + "@6f#X3=cCuncYssPsuRUE"



​    \#生成一个md5对象



​    m1 = hashlib.md5()



​    \#使用md5对象里的update方法md5转换



​    m1.update(md5str.encode("utf-8"))



​    \# 获取加密后的字符



​    token = m1.hexdigest()



​    return token

\```





# day08



1.正则

​    转义字符:

​    \w: 单个的数字字母_

​    \W:

​    \d:

​    \D:

​    \s:

​    \S:



​    特殊的元字符

​    .:

​    *:

​    +:

​    ?: 禁止贪婪

​    

​    (): 子组 隔离 分组

​    | ： 或者

​    []: 字符集的范围

​    {}: {1} {1,} {3,7}

​    ^:

​    $:



2.js逆向工程（解决反扒办法）

​    正向工程



​    md5



反爬：

​    1.header



​    2.登录

​        1.直接登录获取cookie

​        2.模拟post









\---



笔记



1.urllib添加代理



2.requests(urllib) 都是用来发送网络请求的

​    安装 pip install requests

​    

3.



课后练习： 分析 热搜排行接口

# 第三周第二周-回顾

1.代理的使用
    免费代理
        proxy = {
            'http':'http://ip:端口号'
            'https':'https://ip:端口'
        }
    认真代理
        proxy = {
            'http':'http://用户名：密码@ip:端口'
            'https':'https://用户名：密码@ip:端口'
        }

2. requests 第三方模块
   pip install requests

   发送get请求：
   requests.get(url,headers=headers)
   发送post请求
   requests.post(url,data=data,headers=headers)

   响应内容：
       一种直接获取字符串 自动转码 有时会出现乱码
       response.text
       手动指定编码格式
       response.encoding='utf-8'
       获取二进制类型
       response.content

   代理：
       requests.get(url,proxies=proxy)

   模拟表单登录：
       # 创建会话信息   
   ​    requests.session()

3.xpath
    安装：pip install lxml

```
from lxml import etree

response = requests.get()

# 实例xpath对象
html = etree.HTML(response.text)

//
/html/body/div/
# 查找body下的第一个div
/html/body/div[1]
# 通过属性找对象
/html/body/div[@id="item"]

| 或者
/html/body/div[@id="item"]/text()
# 获取 页面当中所有a标签的href属性
//a/@href

//book[price>20]/title/text()
```



# day9 

1.jsonpath
    用来解析json格式的数据
    将json格式的数据按照路径查找的方式去解析

   [
        data:{
            default:{}
        },
        citys:{}
   ]

2. 安装(了解)
   pip install jsonpath

   from jsonpath

   res = jsonpath.jsonpath(dic/list,'规则')
   $
   .
   ..
   [索引1,索引2]
   [起始位置：结束位置：跳步值]
   [?(筛选条件)]
   book[?(8<@.price<20)]

3.beautifulsoup
    安装 pip install beautifulsoup4

```
from bs4 import BeautifulSoup

# 实例对象
html = BeautifulSoup(html字符串,'lxml')

html.select('css选择器')   返回的是列表 就算只有一个元素 也是列表
标签选择器
类选择器
id选择器
子选择器
后代选择器
并列选择器  选择器和选择器之间用'，'隔开 p,a
属性选择器 标签名[属性名]  查找有指定属性名的 指定元素
         标签名[属性名=值]

如何获取文本内容
元素对象.text  获取到文本内容
获取属性：
    元素对象.get(属性名)

find() 返回第一元素对象
find_all()
```



http://www.w3school.com.cn/xpath/xpath_operators.asp   xpath
https://www.cnblogs.com/aoyihuashao/p/8665873.html    jsonpath

任务： 把昨天微博接口信息使用jaonpath提取
糗事百科：

```
https://www.qiushibaike.com/hot/page/1/

提取信息：
    用户名
    性别
    年龄
    段子内容
        如果有图片下载图片 如果没有什么都不用干
    好笑数
    评论数
```









# day10 -16号-老姚（第一天哎）

1. 爬虫五个步骤
   a) 需求分析 
   ------------确定需求（人定的）
   b) 确定网站
   --------------确定资源（人）
   c) 通过URL获取URL的返回值 (urllib, requests)
   --------------通过url获得网站返回的信息（爬虫）
   d) 返回的网站的内容, 获取具体信息(re, XPATH, jsonpath, bs(css selector))
   -----------re必须会,XPATH了解,jsonpath ，bs（重要 css select）
   ----------re工作中，  平时 都是 xpath
   -------描述性数据分析
   -------------定位数据（爬虫）
   e) 存储这些信息(MySQL)
   -------------存储数据（程序）



机器学习(Machine Learning)是一类综合的人工智能算法,涉及1数理统计、2高等数学、3概率论、最优化理论、逼近论、凸分析、神经网络、模式识别等理论。

机器学习算法使计算机通过对已有特征的分析和样本的训练,模拟或实现人类的

学习行为,获得新的知识与技能,从而实现智能的预测分析、数据分类、行为判

断、自然语言理解以及模式识别等功能。当前用于数据挖掘的机器学习算法研究

蓬勃兴起,其核心内容包括:模型训练样本的特征选取;模型的参数训练方法;

以及模型在big-data模式下的实现方式与性能。



```python
hearders = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'
    # referer
    #cookie
    # 剩下的随有hearders
}
```

python3.7不要用虚拟环境。。。。

2. 将来的课程内容
   a) python基础
   b) mysql简单介绍+ 爬虫
   c) MySQL + excel
   d) spss + numpy + pandas + matplotlib
   ~ 考数学
   f) 机器学习
   注：dcf被称为数据分析。

------

后续需要讲的内容:

1. 线程, 进程, 协程
2. selenium
3. 代理的代码写法
4. scrapy
5. 分布式爬虫
6. (足球相关的信息)

------

数学： 线性代数 + 数理统计 （一点）+

线程进程(1. 进程是资源分配的单位, 2. 线程是执行的单位)

1. 进程内部包含所有的程序需要执行的资源(内存, 文件句柄等)
2. 线程几乎不包含任何的资源信息
3. 线程是CPU执行的最小单位
4. 进程中最少有一个线程(进程是包含线程的, 可以有多个线程)

推论:

1. 进程和进程之间的资源是不共享的
2. 同一个进程内的线程之间, 资源是共享的
3. 多进程的程序挂掉一个进程, 其它进程不受影响
4. 同一个进程内的多个线程, 如果进程挂了, 所有内容都没有了

对于编程的指导:

1. 如果两个任务, 之间不共享任何的资源(很少共享资源), 最好用多进程
2. 如果两个任务共享资源比较多, 最好用多线程

另一个推论:
可以有多进程多线程的程序

### 多进程 和 多线程 都是为了增加并发量而准备的

1. 新建进程/线程类
2. 启动进程/线程
3. 等待进程/线程的结束

### 如果想要写多个进程和线程, 就需要重复写多次

进程池:

1. 当我们的任务非常多的时候, 按照以前的方法, 需要创建很多的进程,
   进程创建的过多会发生问题, 就是系统会因为频繁的切换进程, 导致性能下降
   进程池能够限制进程生成的数量
2. 创建许多的进程, 进程的类会被频繁的创建并且销毁, 一样会造成很大的性能问题
   进程池能够重用进程.

### 进程池的代码步骤

1. 创建进程池的类
2. 将任务放入进程池
3. 关闭进程池并等待进程池的结束



# day17日-第11天

1. 协程需要解决的问题(协程的用处)
   a) 线程尽可能多的占用cpu
   b) 能够减少CPU切换线程, 提高CPU的使用效果

老师答：

1.多线程和多进程都可以加快性能的提升；（陈述句）
2 只有有了进程，才会有线程，有了线程才会有协程。cpu对应的是线程，进程通过 线程和协程解决问题---？。
3 一个线程如何提高性能？通过队列和协程来提高性能吗？ 协程切换过于频繁减少切换，可以提高性能



# day18-4

1. ccdi_test_transform2
2. 2. ccdi_test
3. 3. ccdi_test_transform3
4. ccdi
5. proxy_tool
   做了一些练习









# day19-5

------

## scrapy内容

1. 创建一个工程, scrapy 的工程
   scrapy startproject 工程名
2. 在scrapy的工程中, 可以有很多的爬虫程序
3. 生成一个爬虫程序
   scrapy genspider 爬虫名字 需要爬取的域名

http://www.ccdi.gov.cn/special/jdbg3/index.html

4. 执行这个爬虫
   scrapy crawl ccdi
5. 必须知道的是一个调试平台, 叫做scrapy shell

6. windows的安装scrapy步骤
   a) pip install wheel
   b) pip install 你的pywin32路径
   c) pip install 你的Twisted路径
   d) pip install scrapy

验证成功安装scrapy
执行 scrapy 命令

如果使用的是虚拟环境, 请更换成非虚拟环境

7. 如果是mac 或者 linux
   pip install scrapy







## 参考文献