

## Optimization algorithms

机器学习的应用是一个高度依赖经验的过程，伴随着大量迭代的过程，你需要训练诸多模型，才能找到合适的那一个，所以，优化算法能够帮助你快速训练模型。

### 1.1 Mini-bath gradient descent

#### 1.1.1 Batch gradient descent

梯度更新公式：

$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))^2$

$\frac{\partial J(\theta)}{\partial\theta_j} = -\frac1m\sum_{i=0}^m(y^i - 	h_\theta(x^i))x^i_j$

$\theta'_j=\theta_j-\alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$

伪代码：

$Repeat \ until \ convergence \{ \\ \qquad \theta'_j=\theta_j-\alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \}$

优点：

* 全局最优解（每次迭代成本都会下降）
* 易于并行实现（向量化计算一步到位）

缺点：

* 样本数据量很大时，训练过程会很慢

#### 1.1.2 Stochastic gradient descent

随机梯度下降就是每次从所有训练样本中抽取一个样本进行更新，这样不用每次都遍历所有数据集，迭代速度会很快，但是会增加很多迭代次数，因为每一次迭代选取的方向不一定是最优的方向。

SGD因为每次只对一个样本进行梯度下降，所以大部分时候是向着最小值靠近的，但也有一些是离最小值越来越远，因为那些样本恰好指向相反的方向。所以看起来会**有很多噪音，但整体趋势是向最小值逼近**。

伪代码：

$Repeat \ until \ convergence \{ \\ \qquad    for \ i=1 \ to \ m \{            \\   \qquad\qquad \theta'_j=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \qquad\} \\ \}$

优点：

* 训练速度快
* 每次迭代只使用一个训练样本，可用作online learning

缺点：

- 每次只处理一个训练样本，不能通过向量化进行加速

- SGD永远不会收敛，它只会在最小值附近不断的波动，不会到达也不会在此停留

#### 1.1.3 Mini-batch gradient descent

伪代码：

$Repeat \ until \ convergence \{ \\ \qquad    for \ t=1 \ to \ T \{            \\   \qquad\qquad \theta'_j=\theta_j-\alpha\sum_{i=1}^{m_t}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \qquad\} \\ \}$

Batch size选择：

如果训练集较小（一般小于2000时），直接使用Batch gradient descent。

一般mini-batch的大小在64～512之间，选择2的n次幂会运行的相对快一些。

优点：

* pass

缺点：

* pass

#### 1.1.4 Batch vs Stochastic vs Mini-batch

![image-20190614164715917](/image-20190614164715917.png)



### 1.2 Exponentially weighted averages

### 1.3 Gradient descent with Momentum

### 1.4 RMSprop

### 1.5 Adam optiminzation algorithms

```sequence
对象A->对象B: 对象B你好吗?（请求）

Note right of 对象B: 对象B的描述

Note left of 对象A: 对象A的描述(提示)

对象B-->对象A: 我很好(响应)

对象A->对象B: 你真的好吗？

```







### 源代码：

```


## Optimization algorithms

机器学习的应用是一个高度依赖经验的过程，伴随着大量迭代的过程，你需要训练诸多模型，才能找到合适的那一个，所以，优化算法能够帮助你快速训练模型。

### 1.1 Mini-bath gradient descent

#### 1.1.1 Batch gradient descent

梯度更新公式：

$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(y^i-h_\theta(x^i))^2$

$\frac{\partial J(\theta)}{\partial\theta_j} = -\frac1m\sum_{i=0}^m(y^i - 	h_\theta(x^i))x^i_j$

$\theta'_j=\theta_j-\alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$

伪代码：

$Repeat \ until \ convergence \{ \\ \qquad \theta'_j=\theta_j-\alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \}$

优点：

* 全局最优解（每次迭代成本都会下降）
* 易于并行实现（向量化计算一步到位）

缺点：

* 样本数据量很大时，训练过程会很慢

#### 1.1.2 Stochastic gradient descent

随机梯度下降就是每次从所有训练样本中抽取一个样本进行更新，这样不用每次都遍历所有数据集，迭代速度会很快，但是会增加很多迭代次数，因为每一次迭代选取的方向不一定是最优的方向。

SGD因为每次只对一个样本进行梯度下降，所以大部分时候是向着最小值靠近的，但也有一些是离最小值越来越远，因为那些样本恰好指向相反的方向。所以看起来会**有很多噪音，但整体趋势是向最小值逼近**。

伪代码：

$Repeat \ until \ convergence \{ \\ \qquad    for \ i=1 \ to \ m \{            \\   \qquad\qquad \theta'_j=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \qquad\} \\ \}$

优点：

* 训练速度快
* 每次迭代只使用一个训练样本，可用作online learning

缺点：

- 每次只处理一个训练样本，不能通过向量化进行加速

- SGD永远不会收敛，它只会在最小值附近不断的波动，不会到达也不会在此停留

#### 1.1.3 Mini-batch gradient descent

伪代码：

$Repeat \ until \ convergence \{ \\ \qquad    for \ t=1 \ to \ T \{            \\   \qquad\qquad \theta'_j=\theta_j-\alpha\sum_{i=1}^{m_t}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \qquad\} \\ \}$

Batch size选择：

如果训练集较小（一般小于2000时），直接使用Batch gradient descent。

一般mini-batch的大小在64～512之间，选择2的n次幂会运行的相对快一些。

优点：

* pass

缺点：

* pass

#### 1.1.4 Batch vs Stochastic vs Mini-batch

![image-20190614164715917](/image-20190614164715917.png)



### 1.2 Exponentially weighted averages

### 1.3 Gradient descent with Momentum

### 1.4 RMSprop

### 1.5 Adam optiminzation algorithms

​```sequence
对象A->对象B: 对象B你好吗?（请求）

Note right of 对象B: 对象B的描述

Note left of 对象A: 对象A的描述(提示)

对象B-->对象A: 我很好(响应)

对象A->对象B: 你真的好吗？

​```





```

