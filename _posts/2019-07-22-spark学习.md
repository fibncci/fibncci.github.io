---
layout: post
title: "spark--å¤„ç†å¤§æ•°æ®"
date: 2019-09-18
tag: lang
---



## Macç»ˆç«¯è¿è¡Œspark

**é…ç½®ssh**

```
âœ  scala git:(master) âœ— ssh-keygen -t rsa
ä¹‹åä¸€è·¯enteré”®å°±è¡Œï¼›ä¼šæç¤ºæ˜¯å¦è¦†ç›–ä¹‹å‰çš„key,è¾“å…¥yå³å¯ï¼Œä¼šè¿›è¡Œè¦†ç›–ã€‚
Generating public/private rsa key pair.
Enter file in which to save the key (/Users/tianzi/.ssh/id_rsa): 
/Users/tianzi/.ssh/id_rsa already exists.
Overwrite (y/n)? y
Enter passphrase (empty for no passphrase): 
å¯†ç ä¸ºç©º
Enter same passphrase again: 
The key's randomart image is:
+---[RSA 2048]----+
|oo.  E. .=o.     |
|.oo. o .. =.     |
|.oo + + .o.o     |
|=+ o = o oo+ .   |
|*o..=   S . = .  |
|o.o=.  . .   o   |
|o =+ o  .        |
| o .+ .          |
|   .o.           |
+----[SHA256]-----+
âœ  scala git:(master) âœ— cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
ç”¨äºæˆæƒä½ çš„å…¬é’¥åˆ°æœ¬åœ°å¯ä»¥æ— éœ€å¯†ç å®ç°ç™»å½•ã€‚
âœscala git:(ä¸»)âœ—		ssh localhost
æ— æ³•ç¡®å®šä¸»æœº'localhost(::1)'çš„çœŸå®æ€§ã€‚
æ‚¨ç¡®å®šè¦ç»§ç»­è¿æ¥(yes/no)å—?æ˜¯çš„
è­¦å‘Š:åœ¨å·²çŸ¥ä¸»æœºåˆ—è¡¨ä¸­æ°¸ä¹…æ·»åŠ â€œlocalhostâ€(ECDSA)ã€‚
æœ€åç™»å½•æ—¶é—´:2019å¹´9æœˆ18æ—¥æ˜ŸæœŸä¸‰09:35:09
```

é€‰æ‹©ç³»ç»Ÿåå¥½è®¾ç½®->é€‰æ‹©å…±äº«->å‹¾é€‰è¿œç¨‹ç™»å½•ã€‚
ä¹‹åå†æ‰§è¡Œssh localhostå°±å¯ä»¥ç™»å½•æˆåŠŸäº†ã€‚



**2ã€å®‰è£…hadoop**

```scala
âœ  ~ git:(master) âœ— brew install hadoop
Updating Homebrew...
==> Auto-updated Homebrew!
Updated 1 tap (homebrew/core).
ğŸº  /usr/local/Cellar/hadoop/3.1.2: 21,686 files, 774.1MB, built in 3 minutes 21 seconds

æ£€æŸ¥javaç‰ˆæœ¬æ˜¯10.0.1ï¼Œä¼°è®¡æ˜¯å®‰è£…çš„javaç‰ˆæœ¬ä¸ç¬¦åˆè¦æ±‚ã€‚
äºæ˜¯é‡æ–°è¿›è¡Œäº†java JDKçš„å®‰è£…ï¼Œé™„JDKä¸‹è½½åœ°å€ï¼šhttp://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html

3.1é…ç½®hadoop-env.sh
âœ  ~ git:(master) âœ— cd /usr/local/Cellar/hadoop/3.1.2/libexec/etc/hadoop 
å¦‚æœå‡ºé”™ï¼Œåˆ™æ‰‹åŠ¨æ‰¾ï¼›
âœ  ~ git:(master) âœ— cd /usr/local/Cellar/hadoop/3.1.2/libexec/etc/hadoop
3.0.0 å’Œ3.1.2 æ˜¯ç‰ˆæœ¬å·ä¸åŒï¼Œéœ€è¦ä½ è‡ªå·±çœ‹ä¸€ä¸‹ï¼›/usr/local/Cellar/hadoopï¼›
âœ  hadoop vim hadoop-env.sh 

æ‰¾åˆ°hadoop-env.shæ–‡ä»¶ï¼Œå°†export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
æ”¹ä¸ºï¼š
exportHADOOP_OPTS="$HADOOP_OPTS-Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="exportJAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home"
(JDKçš„è·¯å¾„ï¼ŒæŒ‰ç…§è‡ªå·±çš„å®é™…æƒ…å†µè¿›è¡Œé…ç½®å³å¯)


export HADOOP_OPTS="$HADOOP_OPTS-Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="

export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home"



3.2é…ç½®hdfsåœ°å€å’Œç«¯å£
è¿›å…¥hadoopå®‰è£…ç›®å½•ï¼š/usr/local/Cellar/hadoop/3.0.0/libexec/etc/hadoopï¼Œç¼–è¾‘core-site.xmlï¼Œå°†<configuration></configuration>æ”¹ä¸ºï¼š

<configuration>
   <property>
       <name>hadoop.tmp.dir</name>
       <value>/usr/local/Cellar/hadoop/hdfs/tmp</value>
       <description>A base for other temporary directories.</description>
 	 </property>
   <property>
      <name>fs.default.name</name>
      <value>hdfs://localhost:8020</value>
  </property>
</configuration>

3.3  mapreduceä¸­jobtrackerçš„åœ°å€å’Œç«¯å£
ç¼–è¾‘mapred-site.xmlï¼Œå°†<configuration></configuration>æ”¹ä¸ºï¼š
<configuration>
      <property>
              <name>mapred.job.tracker</name>
              <value>localhost:8021</value>
     </property>
</configuration>

3.4  ä¿®æ”¹hdfså¤‡ä»½æ•°
ç¼–è¾‘hdfs-site.xmlï¼Œå°†<configuration></configuration>æ”¹ä¸ºï¼š
<configuration>
       <property>
                <name>dfs.replication</name>
               <value>1</value>
       </property>
</configuration>

3.5  æ ¼å¼åŒ–hdfs;è¿™ä¸ªæ“ä½œç›¸å½“äºä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿçš„åˆå§‹åŒ–
âœ  hadoop hdfs namenode -format
WARNING: /usr/local/Cellar/hadoop/3.1.2/libexec/logs does not exist. Creating.
2019-09-18 10:53:32,311 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = tianmini-2.local/169.254.2.118
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.1.2 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at tianmini-2.local/169.254.2.118
************************************************************/

3.6  é…ç½®Hadoopç¯å¢ƒå˜é‡ 
âœ  hadoop vim ~/.bash_profile
export HADOOP_HOME=/usr/local/Cellar/hadoop/3.1.2/libexec
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

3.7 å¯åŠ¨å…³é—­hadoopæœåŠ¡
è¿›å…¥ç›®å½•ï¼š/usr/local/Cellar/hadoop/3.1.2/libexec/sbinä¸‹ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
./start-all.sh    å¯åŠ¨hadoopå‘½ä»¤
./stop-all.sh   å…³é—­hadoopå‘½ä»¤

âœ  hadoop cd /usr/local/Cellar/hadoop/3.1.2/libexec/sbin      
âœ  sbin ./start-all.sh 
WARNING: Attempting to start all Apache Hadoop daemons as tianzi in 10 seconds.
Starting resourcemanager
Starting nodemanagers

å¯åŠ¨æˆåŠŸåï¼Œåœ¨æµè§ˆå™¨ä¸­è¾“å…¥http://localhost:8088ï¼Œå¯ä»¥çœ‹åˆ°å¦‚ä¸‹é¡µé¢ï¼š
ğŸ˜å¤§è±¡
http://localhost:8088/cluster
```



**4ã€å®‰è£…scala**

```
å‘½ä»¤è¡Œæ‰§è¡Œï¼šbrew install scala
æ‰§è¡Œå®Œæˆåï¼Œå¦‚ä¸‹è¡¨æ˜å®‰è£…æˆåŠŸï¼š
âœ  sbin scala -version
Scala code runner version 2.13.0 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.
âœ  sbin	vim ~/.bash_profileè¿›è¡Œç¼–è¾‘ï¼Œå¢åŠ ç¯å¢ƒå˜é‡ï¼š
export SCALA_HOME=/usr/local/Cellar/scala/2.13.0
export PATH=$PATH:$SCALA_HOME/bin

```

æ³¨ï¼šçœ‹ä¸€ä¸‹è‡ªå·±çš„ç‰ˆæœ¬å· 2.13.0



**5ã€å®‰è£…spark**

```scala
è¿›å…¥Apache Sparkå®˜ç½‘è¿›è¡ŒSparkçš„ä¸‹è½½ï¼Œé™„Sparkå®˜ç½‘ä¸‹è½½åœ°å€ï¼šhttp://spark.apache.org/downloads.html

è§£å‹ä¹‹åï¼Œå¹¶å°†è§£å‹åçš„æ–‡ä»¶å¤¹ç§»åŠ¨åˆ°/usr/local/ç›®å½•ä¸‹ï¼Œ
ç„¶åcd /usr/localè¿›å…¥åˆ°/usr/localç›®å½•ä¸‹ï¼Œ
ä½¿ç”¨å‘½ä»¤æ›´æ”¹è¯¥ç›®å½•ä¸‹çš„sparkæ–‡ä»¶å¤¹åç§°ï¼š å°†æ–‡ä»¶å¤¹åç§°æ”¹ä¸º spark ã€‚
âœ  local sudo mv ./spark-2.4.4-bin-hadoop2.7   ./spark 

5.2  é…ç½®ç¯å¢ƒå˜é‡
âœ  local vim ~/.bash_profile   
å¢åŠ ç¯å¢ƒå˜é‡ï¼š
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
ç„¶åä¿å­˜é€€å‡ºï¼Œä½¿ä¹‹ç”Ÿæ•ˆã€‚
âœ  local source  ~/.bash_profile

5.3  é…ç½®spark-env.sh
(base) âœ  local cd /usr/local/spark/conf
è¿›å…¥åˆ°Sparkç›®å½•çš„confé…ç½®æ–‡ä»¶ä¸­
(base) âœ  conf cp spark-env.sh.template spark-env.sh
å°†spark-env.sh.templateæ‹·è´ä¸€ä»½ï¼Œç„¶åæ‰“å¼€æ‹·è´åçš„spark-env.shæ–‡ä»¶:
åœ¨é‡Œé¢åŠ å…¥å¦‚ä¸‹å†…å®¹:
(base) âœ  conf vim spark-env.sh

export SCALA_HOME=/usr/local/Cellar/scala/2.13.0
export SPARK_MASTER_IP=localhost
export SPARK_WORKER_MEMORY=4G
é…ç½®å¥½ä¹‹åï¼Œå¦‚æœå‡ºç°å¦‚ä¸‹æ‰€ç¤ºçš„ç”»é¢ï¼Œå°±è¡¨æ˜sparkå®‰è£…æˆåŠŸäº†ï¼š
(base) âœ  conf spark-shell


19/09/18 11:44:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://tianmini-2.local:4040
Spark context available as 'sc' (master = local[*], app id = local-1568778278243).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/
         
Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 

```















## Dockeræ­å»ºSparké›†ç¾¤

æœ€è¿‘åœ¨å­¦ä¹ å¤§æ•°æ®æŠ€æœ¯ï¼Œæœ‹å‹å«æˆ‘ç›´æ¥å­¦ä¹ Sparkï¼Œ

è‹±é›„ä¸é—®å‡ºå¤„ï¼Œèœé¸Ÿä¸é—®å¯¹é”™ï¼Œäºæ˜¯æˆ‘å°±å¼€å§‹äº†Sparkå­¦ä¹ ã€‚

**ä¸ºä»€ä¹ˆè¦åœ¨Dockerä¸Šæ­å»ºSparké›†ç¾¤**
Sparkæœ¬èº«æä¾›Localæ¨¡å¼ï¼Œåœ¨å•æœºä¸Šæ¨¡æ‹Ÿå¤šè®¡ç®—èŠ‚ç‚¹æ¥æ‰§è¡Œä»»åŠ¡ã€‚ä½†ä¸çŸ¥é“ä»€ä¹ˆæ€æƒ³åœ¨åšæ€ªï¼Œæ€»è§‰å¾—ä¸æ­å»ºä¸€ä¸ªé›†ç¾¤ï¼Œå¾ˆä¸å®Œç¾çš„æ„Ÿè§‰ã€‚

**æ­å»ºåˆ†å¸ƒå¼é›†ç¾¤ä¸€èˆ¬æœ‰ä¸¤ä¸ªåŠæ³•ï¼š**

æ‰¾å¤šå°æœºå™¨æ¥éƒ¨ç½²ã€‚ï¼ˆå¯¹äºä¸€èˆ¬çš„å­¦ä¹ è€…ï¼Œè¿™ä¸æ˜¯å¾ˆç°å®ï¼Œæˆ‘å°±æ˜¯è¿™ä¸€èˆ¬è¿™ç§ï¼Œæ²¡æœ‰èµ„æºï¼‰
è£…è™šæ‹Ÿæœºï¼Œåœ¨æœ¬åœ°å¼€å¤šä¸ªè™šæ‹Ÿæœºã€‚è¿™å¯¹å®¿ä¸»æœºå™¨æ€§èƒ½è¦æ±‚æ¯”è¾ƒé«˜ï¼Œå› ä¸ºå¤šä¸ªè™šæ‹Ÿæœºå¼€é”€ä¹Ÿå¾ˆå¤§ã€‚åŒæ—¶å®‰è£…å¤šå°è™šæ‹Ÿæœºç€å®è´¹æ—¶éº»çƒ¦ã€‚ï¼ˆå¾ˆå¤šå­¦ä¹ è€…å¯èƒ½ä¼šé€‰æ‹©è¿™ä¸ªåŠæ³•ï¼Œä½†æ˜¯æˆ‘æ€•éº»çƒ¦ï¼Œæˆ‘çš„ç”µè„‘ä¹Ÿä¸å¤ªç»™åŠ›ï¼‰
ä¸Šè¯‰ä¸¤ä¸ªåŠæ³•éƒ½ä¸æ˜¯æˆ‘çš„èœï¼Œæ­£å¥½å‰æ®µæ—¶é—´å¬åŒäº‹èŠå¤©è¯´åˆ°Dockerã€‚

**ç™¾åº¦ç™¾ç§‘å¯¹Dockerçš„è§£é‡Šå¦‚ä¸‹ï¼š**

Docker æ˜¯ä¸€ä¸ªå¼€æºçš„åº”ç”¨å®¹å™¨å¼•æ“ï¼Œè®©å¼€å‘è€…å¯ä»¥æ‰“åŒ…ä»–ä»¬çš„åº”ç”¨ä»¥åŠä¾èµ–åŒ…åˆ°ä¸€ä¸ªå¯ç§»æ¤çš„å®¹å™¨ä¸­ï¼Œç„¶åå‘å¸ƒåˆ°ä»»ä½•æµè¡Œçš„ Linux æœºå™¨ä¸Šï¼Œä¹Ÿå¯ä»¥å®ç°è™šæ‹ŸåŒ–ã€‚å®¹å™¨æ˜¯å®Œå…¨ä½¿ç”¨æ²™ç®±æœºåˆ¶ï¼Œç›¸äº’ä¹‹é—´ä¸ä¼šæœ‰ä»»ä½•æ¥å£ã€‚

äºæ˜¯å°±æƒ³åˆ°ä½¿ç”¨Dockeræ¥éƒ¨ç½²Sparké›†ç¾¤äº†ã€‚

```
ä»0å¼€å§‹æ­å»ºSparké›†ç¾¤
å®‰è£…ubuntuè™šæ‹Ÿæœº
æˆ‘çš„æœºå™¨æ˜¯ Macbook Pro 13.3å¯¸ï¼Œè¿™é‡Œæˆ‘é€‰æ‹©çš„è™šæ‹Ÿæœºæ˜¯VMware Fusion 8.5.8ï¼Œubuntu 16.04
å…·ä½“å®‰è£…è¿‡ç¨‹å°±ä¸è®°å½•äº†ï¼Œéå¸¸ç®€å•ï¼Œä¹Ÿä¸éœ€è¦æ•™ç¨‹ã€‚ä½†åœ¨è¿™é‡Œè¦é’ˆå¯¹MBPçš„å®‰è£…è™šæ‹Ÿæœºçš„é…ç½®åšå‡ ç‚¹è®°å½•ï¼š

åˆ†è¾¨ç‡
è™šæ‹Ÿæœºçš„é…ç½®ï¼Œå¿…é¡»é€‰æ‹©å¦‚å›¾æ‰€ç¤ºï¼Œä¸ç„¶ubuntuçš„UIä¼šå‘è™š;
ç„¶åï¼Œæ‰“å¼€å¯åŠ¨ubuntuåï¼Œä¼šå‘ç°å›¾æ ‡å’Œæ–‡å­—éƒ½éå¸¸å°ã€‚åŸå› æ˜¯ubuntuçš„åˆ†è¾¨ç‡ä½¿ç”¨äº†å®¿ä¸»æœºMBPçš„åˆ†è¾¨ç‡ï¼Œç¼©æ”¾scale=1. åœ¨è¿™é‡Œè¦é’ˆå¯¹ubuntuçš„åˆ†è¾¨ç‡åšè®¾ç½®ï¼Œå°†scaleè®¾ç½®ä¸º2ï¼Œ

VMware Fusionè™šæ‹ŸæœºUbuntuä¸­å®ç°ä¸ä¸»æœºå…±äº«ï¼ˆå¤åˆ¶å’Œç²˜è´´ï¼‰
åœ¨Ubuntuèœå•ä¸Šé€‰æ‹©VM->install VMware toolsã€‚ç„¶åå‡ºç°VMware toolsçš„å®‰è£…å‹ç¼©åŒ…æ–‡ä»¶VMwareTools-9.2.0-799703.tar.gzã€‚
åœ¨ç»ˆç«¯ä½¿ç”¨å‘½ä»¤ tar xvzf VMwareTools-9.2.0-799703.tar.gz /root/vm è§£å‹
å‘½ä»¤ sudo ./root/vm/vmware-install.pl è¿è¡Œè§£å‹åçš„ç›®å½•é‡Œçš„vmware-install.plæ–‡ä»¶è¿›è¡Œå®‰è£…
å®Œæˆï¼Œé‡å¯ã€‚ å‘½ä»¤ reboot
rootç”¨æˆ·ç™»å½•
ç”±äºåé¢çš„å¾ˆå¤šæ“ä½œåŸºæœ¬éƒ½æ˜¯éœ€è¦rootæƒé™çš„ï¼Œåœ¨è¿™é‡Œæ˜¯ä¸ºäº†å­¦ä¹ çš„ï¼Œæ‰€ä»¥ç´¢æ€§ä»¥rootç”¨æˆ·ç™»å½•ï¼Œä½†æ˜¯ç”±äºé»˜è®¤çš„ç™»å½•ç•Œé¢æ²¡æœ‰rootç”¨æˆ·ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œè¦åšä¸€ä¸‹é…ç½®.

é‡ç½®rootç”¨æˆ·å¯†ç ï¼Œåœ¨ç»ˆç«¯è¾“å…¥ sudo passwd root ï¼Œä¸€è·¯è¾“å…¥æ–°å¯†ç å³å¯
åœ¨ç³»ç»Ÿç™»å½•ç•Œé¢ä»¥ root ç”¨æˆ·ç™»å½•
vim /usr/share/lightdm/lightdm.conf.d/50-unity-greeter.conf æ‰“å¼€50-unity-greeter.confæ–‡ä»¶ï¼Œå¹¶åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ï¼š
user-session=ubuntu
greeter-show-manual-login=true
all-guest=false 


ä¿å­˜é€€å‡º

ç»ˆç«¯è¾“å…¥ vim /root/.profileå‘½ä»¤ æ‰“å¼€ .profile æ–‡ä»¶ï¼Œå¹¶ä¿®æ”¹å…¶æœ€åä¸€è¡Œä¸ºtty -s && mesg n || true


ä¿å­˜é€€å‡ºã€‚
é‡å¯ç³»ç»Ÿï¼Œrebootã€‚
ç„¶ååœ¨ç™»å½•é¡µé¢ Login usernameå¤„å¯ä»¥è¾“å…¥ rootï¼Œ å›è½¦ï¼Œç„¶åè¾“å…¥å¯†ç å³å¯ç™»å½• root ç”¨æˆ·
ubuntuè™šæ‹Ÿæœºçš„å®‰è£…å’Œé…ç½®åˆ°æ­¤åŸºæœ¬å®Œæˆäº†ã€‚ä¸‹é¢å°±æ˜¯æ­£å¼è¿›å…¥Dockeré›†ç¾¤çš„æ­å»ºäº†ã€‚

Docker å®‰è£…
Docker æ˜¯ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆç”¨ï¼Ÿåœ¨è¿™é‡Œå°±ä¸æŠ„è¿‡æ¥äº†ï¼Œå› ä¸ºæˆ‘ä»¬è¿™é‡Œä¸»è¦æ˜¯ä½¿ç”¨Dockeræ¥éƒ¨ç½²Sparké›†ç¾¤çš„ï¼Œæˆ‘å°±æš‚æ—¶ä¸æ·±ç©¶äº†ï¼Œä»¥åå†æ·±å…¥äº†è§£ä¸‹ã€‚é™„Docker ç™¾ç§‘

æˆ‘ä½¿ç”¨çš„æ˜¯ ubuntu 16.04 , å…·ä½“å®‰è£…å‘½ä»¤å¦‚ä¸‹ï¼š

$ apt update
$ apt-get install apt-transport-https
$ apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9
$ bash -c "echo deb https://get.docker.io/ubuntu docker main > /etc/apt/sources.list.d/docker.list"
$ apt update
$ apt install lxc-docker
å®‰è£…å®Œæˆåï¼Œæ ¡éªŒæ˜¯å¦å®‰è£…æˆåŠŸ

ä½¿ç”¨ docker version å‘½ä»¤ï¼Œå¦‚æœè¾“å‡ºå¦‚ä¸‹ä¿¡æ¯ï¼Œè¯æ˜æˆåŠŸå®‰è£…ï¼š


ä¸‹è½½ubuntu é•œåƒ
$ docker pull ubuntu:16.04
è¿™æ¡å‘½ä»¤çš„ä½œç”¨æ˜¯ä»Dockerä»“åº“ä¸­è·å–ubuntuçš„é•œåƒ

ä¸‹è½½å®Œæˆåï¼Œä½¿ç”¨ä¸‹é¢å‘½ä»¤å¯ä»¥åˆ—å‡ºæ‰€æœ‰æœ¬åœ°çš„é•œåƒï¼š

$ docker images
å¦‚å›¾ï¼š

å½“ç„¶ï¼Œå›¾ä¸­çš„å¦å¤–ä¸€æ¡ ubuntu:hadoop æ˜¯æˆ‘å®‰è£…äº†hadoop/sparké›†ç¾¤åæ‰“çš„tag

å¯åŠ¨ä¸€ä¸ªå®¹å™¨
ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¯åŠ¨ä¸€ä¸ªå®¹å™¨

$ docker run -ti ubuntu:16.04
å¯åŠ¨ä¸€ä¸ªå®¹å™¨åï¼Œå°†çœ‹åˆ°å¦‚å›¾æ•ˆæœï¼š
3.png

å®¹å™¨å¯åŠ¨åï¼Œæ¥ä¸‹æ¥å°±è¦å®‰è£…javaï¼ŒåŠè¿›è¡Œç›¸å…³é…ç½®
å®‰è£… java
ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ä¸€ä¸ªjavaç¯å¢ƒ

$ apt update
$ apt install software-properties-common python-software-properties
$ add-apt-repository ppa:webupd8team/java
$ apt update
$ apt install oracle-java8-installer
$ java -version
$ apt install oracle-java8-installer è¿™æ¡å‘½ä»¤æœ‰å¯èƒ½å¤±è´¥ï¼Œå¤šè¯•å‡ æ¬¡å°±å¯ä»¥äº†
åœ¨ $ java -version å‘½ä»¤åä½ å°†çœ‹åˆ°javaçš„ç‰ˆæœ¬ä¿¡æ¯
å¦‚å›¾ï¼š
4.png
å®‰è£…å®Œ java åï¼Œä¸‹é¢å°±è¦è¿›è¡Œç¯å¢ƒå˜é‡çš„é…ç½®äº†
ä¿®æ”¹~/.bashrcæ–‡ä»¶ã€‚åœ¨æ–‡ä»¶æœ«å°¾åŠ å…¥ä¸‹é¢é…ç½®ä¿¡æ¯ï¼š
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
å¦‚å›¾ï¼š

æ¥ä¸‹æ¥æ˜¯å®‰è£…é›†ç¾¤äº†ï¼ŒåŒ…æ‹¬zookeeperã€hadoopã€spark.
æ¥ä¸‹æ¥çš„å·¥ä½œå¯èƒ½ä¼šç”¨åˆ°å¦‚ä¸‹å‘½ä»¤ï¼š

wget http://... ï¼Œç”¨äºä¸‹è½½èµ„æºæ–‡ä»¶
ifconfig ç”¨äºæŸ¥çœ‹å½“å‰å®¹å™¨ipä¿¡æ¯
vim ç”¨äºç¼–è¾‘æ–‡ä»¶
æ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œå¯ä»¥å…ˆè¿›è¡Œå®‰è£…è¿™äº›å·¥å…·:

$ apt update
$ apt install wget
$ apt install vim
$ apt install net-tools       # ifconfig 
$ apt install iputils-ping     # ping
éƒ½å®‰è£…å¥½åï¼Œå¯ä»¥å°†æ­¤è£…å¥½ç¯å¢ƒå˜é‡çš„é•œåƒä¿å­˜ä¸ºä¸€ä¸ªå‰¯æœ¬ï¼Œä»¥åå¯ä»¥åŸºäºæ­¤å‰¯æœ¬æ„å»ºå…¶å®ƒé•œåƒï¼š

docker commit -m "java install" 009cf5ac0834 ubuntu:hadoop
å¦‚å›¾ï¼š
1.png

ç„¶åä½¿ç”¨ docker images å‘½ä»¤å°†ä¼šçœ‹åˆ°ä¿å­˜å¥½çš„æœ¬åœ°å‰¯æœ¬ï¼š
2.png
å®‰è£… Zookeeperã€ Hadoopã€Sparkã€Scala
ä¸‹è½½é›†ç¾¤èµ„æº
æˆ‘ä»¬è®¡åˆ’å°†é›†ç¾¤çš„ Zookeeperã€Hadoopã€Spark å®‰è£…åˆ°ç»Ÿä¸€çš„ç›®å½• /root/soft/apacheä¸‹ã€‚
æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬è¦å…ˆæ„å»ºè¿™ä¸ªç›®å½•ï¼š

$ cd ~/
$ mkdir soft
$ cd soft
$ mkdir apache
$ mkdir scala  #è¿™ä¸ªç›®å½•æ˜¯ç”¨æ¥å®‰è£… scala çš„
$ cd apache
$ mkdir zookeeper
$ mkdir hadoop
$ mkdir spark
ä¸‹è½½ zookeeper
ç„¶ååˆ°è¿™é‡Œä¸‹è½½ zookeeper åˆ° /root/soft/apache/zookeeper ç›®å½•ä¸‹, æˆ‘è¿™é‡Œä¸‹è½½çš„æ˜¯ zookeeper-3.4.9

$ cd /root/soft/apache/zookeeper
$ wget http://archive.apache.org/dist/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz
ä¸‹è½½ hadoop
ç„¶ååˆ°è¿™é‡Œä¸‹è½½ hadoop åˆ° /root/soft/apache/hadoop ç›®å½•ä¸‹, æˆ‘è¿™é‡Œä¸‹è½½çš„æ˜¯ hadoop-2.7.4

$ cd /root/soft/apache/hadoop
$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz
ä¸‹è½½ spark
ç„¶ååˆ°è¿™é‡Œä¸‹è½½ spark åˆ° /root/soft/apache/spark ç›®å½•ä¸‹, æˆ‘è¿™é‡Œä¸‹è½½çš„æ˜¯ spark-2.2.0-bin-hadoop2.7

$ cd /root/soft/apache/spark
$ wget https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz
ä¸‹è½½ scala
ç„¶ååˆ°è¿™é‡Œä¸‹è½½ scala åˆ° /root/soft/scala ç›®å½•ä¸‹, æˆ‘è¿™é‡Œä¸‹è½½çš„æ˜¯ scala-2.11.11

$ cd /root/soft/scala
$ wget https://downloads.lightbend.com/scala/2.11.11/scala-2.11.11.tgz
å®‰è£… Zookeeper
è¿›å…¥ zookeeper ç›®å½•ï¼Œç„¶åè§£å‹ä¸‹è½½ä¸‹æ¥çš„ zookeeper-3.4.9.tar.gz
$ cd /root/soft/apache/zookeeper
$ tar xvzf zookeeper-3.4.9.ar.gz
ä¿®æ”¹ ~/.bashrc, é…ç½® zookeeper ç¯å¢ƒå˜é‡
$ vim ~/.bashrc 
   export ZOOKEEPER_HOME=/root/soft/apache/zookeeper/zookeeper-3.4.9
   export PATH=$PATH:$ZOOKEEPER_HOME/bin
$ source ~/.bashrc #ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
å¦‚å›¾ï¼š


1.png
ä¿®æ”¹ zookeeper é…ç½®ä¿¡æ¯ï¼š
$ cd zookeeper-3.4.9/conf/
$ cp zoo_sample.cfg zoo.cfg
$ vim zoo.cfg
ä¿®æ”¹å¦‚ä¸‹ä¿¡æ¯ï¼š

dataDir=/root/soft/apache/zookeeper/zookeeper-3.4.9/tmp
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
å¦‚å›¾ï¼š


2.png
æ¥ä¸‹æ¥æ·»åŠ  myid æ–‡ä»¶
$ cd ../
$ mkdir tmp
$ cd tmp
$ touch myid
$ echo 1 > myid
..../tmp/myid æ–‡ä»¶ä¸­ä¿å­˜çš„æ•°å­—ä»£è¡¨æœ¬æœºçš„zkServerç¼–å· åœ¨æ­¤è®¾ç½®masterä¸ºç¼–å·ä¸º1çš„zkServerï¼Œä¹‹åç”Ÿæˆslave1å’Œslave2ä¹‹åè¿˜éœ€è¦åˆ†åˆ«ä¿®æ”¹æ­¤æ–‡ä»¶

å®‰è£… Hadoop
è¿›å…¥ hadoop ç›®å½•ï¼Œç„¶åè§£å‹ä¸‹è½½ä¸‹æ¥çš„ hadoop-2.7.4.tar.gz
$ cd /root/soft/apache/hadoop
$ tar xvzf hadoop-2.7.4.tar.gz
ä¿®æ”¹ ~/.bashrc, é…ç½® hadoop ç¯å¢ƒå˜é‡
$ vim ~/.bashrc
     export HADOOP_HOME=/root/soft/apache/hadoop/hadoop-2.7.4
     export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop
     export PATH=$PATH:$HADOOP_HOME/bin
     export PATH=$PATH:$HADOOP_HOME/sbin
   # ä¿å­˜é€€å‡º esc :wq!
$ source ~/.bashrc #ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
å¦‚å›¾ï¼š


1.png
é…ç½® hadoop
# é¦–å…ˆè¿›å…¥ `hadoop` é…ç½®æ–‡ä»¶çš„ç›®å½•ï¼Œå› ä¸º `hadoop` æ‰€æœ‰çš„é…ç½®éƒ½åœ¨æ­¤ç›®å½•ä¸‹
$ cd $HADOOP_CONFIG_HOME/
ä¿®æ”¹æ ¸å¿ƒé…ç½® core-site.xml, æ·»åŠ å¦‚ä¸‹ä¿¡æ¯åˆ°æ­¤æ–‡ä»¶çš„< configuration > </configuration > ä¸­é—´
<configuration>
    <property>
         <name>hadoop.tmp.dir</name>
         <value>/root/soft/apache/hadoop/hadoop-2.7.4/tmp</value>
         <description>A base for other temporary directories.</description>
     </property>
     <property>
         <name>fs.default.name</name>
         <value>hdfs://master:9000</value>
         <final>true</final>
         <description>The name of the default file system.  A URI whose scheme and authority determine the FileSystem implementation.  The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class.  The uri's authority is used to determine the host, port, etc. for a filesystem.</description>
      </property>
<configuration>
ä¿®æ”¹ hdfs-site.xml, æ·»åŠ å¦‚ä¸‹ä¿¡æ¯ï¼š
# dfs.nameservices åç§°æœåŠ¡ï¼Œåœ¨åŸºäºHAçš„HDFSä¸­ï¼Œç”¨åç§°æœåŠ¡æ¥è¡¨ç¤ºå½“å‰æ´»åŠ¨çš„NameNode
# dfs.ha.namenodes. é…ç½®åç§°æœåŠ¡ä¸‹æœ‰å“ªäº›NameNode 
# dfs.namenode.rpc-address.. é…ç½®NameNodeè¿œç¨‹è°ƒç”¨åœ°å€ 
# dfs.namenode.http-address.. é…ç½®NameNodeæµè§ˆå™¨è®¿é—®åœ°å€ 
# dfs.namenode.shared.edits.dir é…ç½®åç§°æœåŠ¡å¯¹åº”çš„JournalNode 
# dfs.journalnode.edits.dir JournalNodeå­˜å‚¨æ•°æ®çš„è·¯å¾„

<configuration>
<property>
   <name>dfs.nameservices</name>
   <value>ns1</value>
</property>
<property>
   <name>dfs.ha.namenodes.ns1</name>
   <value>nn1,nn2</value>
</property>
<property>
   <name>dfs.namenode.rpc-address.ns1.nn1</name>
   <value>master:9000</value>
</property>
<property>
   <name>dfs.namenode.http-address.ns1.nn1</name>
   <value>master:50070</value>
</property>
<property>
   <name>dfs.namenode.rpc-address.ns1.nn2</name>
   <value>slave1:9000</value>
</property>
<property>
   <name>dfs.namenode.http-address.ns1.nn2</name>
   <value>slave1:50070</value>
</property>
<property>
   <name>dfs.namenode.shared.edits.dir</name>
<value>qjournal://master:8485;slave1:8485;slave2:8485/ns1</value>
</property>
<property>
   <name>dfs.journalnode.edits.dir</name>
   <value>/root/soft/apache/hadoop/hadoop-2.7.4/journal</value>
</property>
<property>
   <name>dfs.ha.automatic-failover.enabled</name>
   <value>true</value>
</property>
<property>
   <name>dfs.client.failover.proxy.provider.ns1</name>
   <value>
   org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
   </value>
</property>
<property>
   <name>dfs.ha.fencing.methods</name>
   <value>
   sshfence
   shell(/bin/true)
   </value>
</property>
<property>
   <name>dfs.ha.fencing.ssh.private-key-files</name>
   <value>/root/.ssh/id_rsa</value>
</property>
<property>
   <name>dfs.ha.fencing.ssh.connect-timeout</name>
   <value>30000</value>
</property>
</configuration>
ä¿®æ”¹ Yarn çš„é…ç½®æ–‡ä»¶yarn-site.xmlï¼š
# yarn.resourcemanager.hostname RescourceManagerçš„åœ°å€ï¼ŒNodeManagerçš„åœ°å€åœ¨slavesæ–‡ä»¶ä¸­å®šä¹‰

<configuration>
<!-- Site specific YARN configuration properties -->
<property>
   <name>yarn.resourcemanager.hostname</name>
   <value>master</value>
</property>
<property>
   <name>yarn.nodemanager.aux-services</name>
   <value>mapreduce_shuffle</value>
</property>
</configuration>
ä¿®æ”¹ mapred-site.xml
è¿™ä¸ªæ–‡ä»¶æ˜¯ä¸å­˜åœ¨çš„ï¼Œéœ€è¦å°† mapred-site.xml.template copyä¸€ä»½
$ cp mapred-site.xml.template mapred-site.xml
ç„¶åç¼–è¾‘ mapred-site.xml ï¼Œæ·»åŠ å¦‚ä¸‹ä¿¡æ¯åˆ°æ–‡ä»¶

<configuration>
<!-- æŒ‡å®šMapReduceæ¡†æ¶ä¸ºyarnæ–¹å¼ -->
<property>
    <name>
      mapreduce.framework.name
    </name>
    <value>yarn</value>
</property>
</configuration>
ä¿®æ”¹æŒ‡å®š DataNode å’Œ NodeManager çš„é…ç½®æ–‡ä»¶ slaves :
$ vim slaves
æ·»åŠ å¦‚ä¸‹èŠ‚ç‚¹å

master
slave1
slave2
åˆ°æ­¤ hadoop ç®—æ˜¯å®‰è£…é…ç½®å¥½äº†

å®‰è£… Spark
è¿›å…¥ spark ç›®å½•ï¼Œç„¶åè§£å‹ä¸‹è½½ä¸‹æ¥çš„ spark-2.2.0-bin-hadoop2.7.tgz
$ cd /root/soft/apache/spark
$ tar xvzf spark-2.2.0-bin-hadoop2.7.tgz
ä¿®æ”¹ ~/.bashrc, é…ç½® spark ç¯å¢ƒå˜é‡
$ vim ~/.bashrc
    export SPARK_HOME=/root/soft/apache/spark/spark-2.2.0-bin-hadoop2.7
    export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
   # ä¿å­˜é€€å‡º esc :wq!
$ source ~/.bashrc #ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
ä¿®æ”¹ spark é…ç½®
$ cd spark-2.2.0-bin-hadoop2.7/conf
$ cp spark-env.sh.template spark-env.sh
$ vim spark-env.sh
æ·»åŠ å¦‚ä¸‹ä¿¡æ¯ï¼š

export SPARK_MASTER_IP=master
export SPARK_WORKER_MEMORY=128m
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export SCALA_HOME=/root/soft/scala/scala-2.11.11  # scalaæˆ‘ä»¬åé¢ä¼šå®‰è£…å®ƒ
export SPARK_HOME=/root/soft/apache/spark/spark-2.2.0-bin-hadoop2.7
export HADOOP_CONF_DIR=/root/soft/apache/hadoop/hadoop-2.7.4/etc/hadoop
export SPARK_LIBRARY_PATH=$SPARK_HOME/lib
export SCALA_LIBRARY_PATH=$SPARK_LIBRARY_PATH
export SPARK_WORKER_CORES=1
export SPARK_WORKER_INSTANCES=1
export SPARK_MASTER_PORT=7077
ä¿å­˜é€€å‡º esc :wq!

ä¿®æ”¹æŒ‡å®šWorkerçš„é…ç½®æ–‡ä»¶ slavesï¼š

$ vim slaves
æ·»åŠ 

master
slave1
slave2
åˆ°è¿™é‡Œï¼ŒSpark ä¹Ÿç®—å®‰è£…é…ç½®å®Œæˆäº†ã€‚

å®‰è£… Scala
è¿›å…¥ scala ç›®å½•ï¼Œç„¶åè§£å‹ä¸‹è½½ä¸‹æ¥çš„ scala-2.11.11.tgz
$ cd /root/soft/scala
$ tar xvzf scala-2.11.11.tgz
ä¿®æ”¹ ~/.bashrc, é…ç½® spark ç¯å¢ƒå˜é‡
$ vim ~/.bashrc
    export SCALA_HOME=/root/soft/scala/scala-2.11.11
   export PATH=$PATH:$SCALA_HOME/bin
   # ä¿å­˜é€€å‡º esc :wq!
$ source ~/.bashrc #ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
åˆ°è¿™é‡Œï¼Œscala ä¹Ÿç®—å®‰è£…é…ç½®å®Œæˆäº†

å®‰è£… SSH, é…ç½®æ— å¯†ç è®¿é—®é›†ç¾¤å…¶å®ƒæœºå™¨
æ­å»ºé›†ç¾¤ç¯å¢ƒï¼Œè‡ªç„¶å°‘ä¸äº†ä½¿ç”¨SSHã€‚è¿™å¯ä»¥å®ç°æ— å¯†ç è®¿é—®ï¼Œè®¿é—®é›†ç¾¤æœºå™¨çš„æ—¶å€™å¾ˆæ–¹ä¾¿ã€‚
ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£… ssh

$ apt install ssh 
SSHè£…å¥½äº†ä»¥åï¼Œç”±äºæˆ‘ä»¬æ˜¯ Docker å®¹å™¨ä¸­è¿è¡Œï¼Œæ‰€ä»¥ SSH æœåŠ¡ä¸ä¼šè‡ªåŠ¨å¯åŠ¨ã€‚éœ€è¦æˆ‘ä»¬åœ¨å®¹å™¨å¯åŠ¨ä»¥åï¼Œæ‰‹åŠ¨é€šè¿‡/usr/sbin/sshd æ‰‹åŠ¨æ‰“å¼€SSHæœåŠ¡ã€‚æœªå…æœ‰äº›éº»çƒ¦ï¼Œä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªå‘½ä»¤åŠ å…¥åˆ°~/.bashrcæ–‡ä»¶ä¸­ã€‚é€šè¿‡vim ~/.bashrcç¼–è¾‘.bashrcæ–‡ä»¶,

$ vim ~/.bashrc
åœ¨æ–‡ä»¶åè¿½åŠ ä¸‹é¢å†…å®¹ï¼š

#autorun
/usr/sbin/sshd
ç„¶åè¿è¡Œ source ~/.bashrc ä½¿é…ç½®ç”Ÿæ•ˆ

$ source ~/.bashrc
æ­¤è¿‡ç¨‹å¯èƒ½ä¼šæŠ¥é”™:
Missing privilege separation directory: /var/run/sshd éœ€è¦è‡ªå·±åˆ›å»ºè¿™ä¸ªç›®å½•

$ mkdir /var/run/sshd
ç”Ÿæˆè®¿é—®å¯†é’¥
$ cd ~/
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$ cd .ssh
$ cat id_rsa.pub >> authorized_keys
æ³¨æ„ï¼š è¿™é‡Œï¼Œæˆ‘çš„æ€è·¯æ˜¯ç›´æ¥å°†å¯†é’¥ç”Ÿæˆåå†™å…¥é•œåƒï¼Œå…å¾—åœ¨ä¹°ä¸ªå®¹å™¨é‡Œé¢å†å•ç‹¬ç”Ÿæˆä¸€æ¬¡ï¼Œè¿˜è¦ç›¸äº’æ‹·è´å…¬é’¥ï¼Œæ¯”è¾ƒéº»çƒ¦ã€‚å½“ç„¶è¿™åªæ˜¯å­¦ä¹ ä½¿ç”¨ï¼Œå®é™…æ“ä½œæ—¶ï¼Œåº”è¯¥ä¸ä¼šè¿™ä¹ˆæï¼Œå› ä¸ºè¿™æ ·æ‰€æœ‰å®¹å™¨çš„å¯†é’¥éƒ½æ˜¯ä¸€æ ·çš„ï¼ï¼ï¼

åˆ°è¿™é‡Œï¼ŒSSH ä¹Ÿç®—å®‰è£…é…ç½®å®Œæˆäº†

åˆ°è¿™é‡Œï¼ŒSpark é›†ç¾¤ç®—æ˜¯åŸºæœ¬å®‰è£…é…ç½®å¥½äº†ï¼Œå‰©ä¸‹å°±æ˜¯éƒ¨ç½²åˆ†å¸ƒå¼äº†ã€‚

å…ˆæŸ¥çœ‹ip

 $ ifconfig
 #//172.17.0.2
ä¿å­˜é•œåƒå‰¯æœ¬
è¿™é‡Œæˆ‘ä»¬å°†å®‰è£…å¥½Zookeeperã€ Hadoppã€ Sparkã€Scala çš„é•œåƒä¿å­˜ä¸ºä¸€ä¸ªå‰¯æœ¬

é€€å‡º Docker
$ exit 
ä¿å­˜ä¸€ä¸ªå‰¯æœ¬
$ docker commit -m "zookeeper hadoop spark scala install" 7b97ba289b22 ubuntu:spark
ä¹‹åæˆ‘ä»¬ä¼šåŸºäºæ­¤å‰¯æœ¬æ¥è¿è¡Œæˆ‘ä»¬çš„é›†ç¾¤

åˆ†å¸ƒå¼å„èŠ‚ç‚¹å¯åŠ¨è„šæœ¬
éªŒè¯ä¸€ä¸‹ IP è§„åˆ™
åˆ†åˆ«å¼€ 3 ä¸ªç»ˆç«¯ã€‚åˆ†åˆ«è·‘å¦‚ä¸‹å‘½ä»¤ï¼Œå¼€å¯3 ä¸ªDocker
ç»ˆç«¯ 1:

$ docker run -ti -h master ubuntu:spark
$ ifconfig #172.17.0.2
ç»ˆç«¯ 2:

$ docker run -ti -h slave1 ubuntu:spark
$ ifconfig #172.17.0.3
ç»ˆç«¯ 3:

$ docker run -ti -h slave2 ubuntu:spark
$ ifconfig #172.17.0.4
çœ‹åˆ°äº†æ²¡ï¼Œè¿™3ä¸ªDockerçš„ ip åˆ†åˆ«æ˜¯172.17.0.2ã€ 172.17.0.3 ã€172.17.0.4ï¼Œå®ƒæ˜¯å–å†³äºå¯åŠ¨Docker çš„é¡ºåºçš„ã€‚
æ¥ä¸‹æ¥é€€å‡ºè¿™å‡ ä¸ªDockerï¼Œç„¶åç¼–å†™å¯åŠ¨è„šæœ¬

ç¼–å†™é›†ç¾¤èŠ‚ç‚¹å¯åŠ¨è„šæœ¬
å¯åŠ¨ ubuntu:spark
$ docker run -ti ubuntu:spark
è¿›å…¥ /root/soft ç›®å½•ï¼Œæˆ‘ä»¬å°†å¯åŠ¨è„šæœ¬éƒ½æ”¾è¿™é‡Œå§

$ cd /root/soft
$ mkdir shell
$ cd shell
vim run_master.sh åˆ›å»º Master èŠ‚ç‚¹çš„è¿è¡Œè„šæœ¬
$ vim run_master.sh 
æ·»åŠ å¦‚ä¸‹ä¿¡æ¯ï¼š

#!/bin/bash
#æ¸…ç©ºhostsæ–‡ä»¶ä¿¡æ¯
echo> /etc/hosts
#é…ç½®ä¸»æœºçš„host
echo 172.17.0.1 host >> /etc/hosts
echo 172.17.0.2 master >> /etc/hosts
echo 172.17.0.3 slave1 >> /etc/hosts
echo 172.17.0.4 slave2 >> /etc/hosts

#é…ç½® master èŠ‚ç‚¹çš„ zookeeper çš„ server id
echo 1 > /root/soft/apache/zookeeper/zookeeper-3.4.9/tmp/myid

zkServer.sh start

hadoop-daemons.sh start journalnode
hdfs namenode -format
hdfs zkfc -formatZK

start-dfs.sh
start-yarn.sh
start-all.sh
vim run_slave1.sh åˆ›å»º Slave1 èŠ‚ç‚¹çš„è¿è¡Œè„šæœ¬
$ vim run_slave1.sh 
æ·»åŠ å¦‚ä¸‹ä¿¡æ¯ï¼š

#!/bin/bash
#æ¸…ç©ºhostsæ–‡ä»¶ä¿¡æ¯
echo> /etc/hosts
#é…ç½®ä¸»æœºçš„host
echo 172.17.0.1 host >> /etc/hosts
echo 172.17.0.2 master >> /etc/hosts
echo 172.17.0.3 slave1 >> /etc/hosts
echo 172.17.0.4 slave2 >> /etc/hosts

#é…ç½® master èŠ‚ç‚¹çš„ zookeeper çš„ server id
echo 2 > /root/soft/apache/zookeeper/zookeeper-3.4.9/tmp/myid

zkServer.sh start
vim run_slave2.sh åˆ›å»º Slave2 èŠ‚ç‚¹çš„è¿è¡Œè„šæœ¬
$ vim run_slave2.sh 
æ·»åŠ å¦‚ä¸‹ä¿¡æ¯ï¼š

#!/bin/bash
#æ¸…ç©ºhostsæ–‡ä»¶ä¿¡æ¯
echo> /etc/hosts
#é…ç½®ä¸»æœºçš„host
echo 172.17.0.1 host >> /etc/hosts
echo 172.17.0.2 master >> /etc/hosts
echo 172.17.0.3 slave1 >> /etc/hosts
echo 172.17.0.4 slave2 >> /etc/hosts

#é…ç½® master èŠ‚ç‚¹çš„ zookeeper çš„ server id
echo 3 > /root/soft/apache/zookeeper/zookeeper-3.4.9/tmp/myid

zkServer.sh start
vim stop_master.sh åˆ›å»º Stop è„šæœ¬
$ vim stop_master.sh 
æ·»åŠ å¦‚ä¸‹ä¿¡æ¯ï¼š

#!/bin/bash
zkServer.sh stop
hadoop-daemons.sh stop journalnode
stop-dfs.sh
stop-yarn.sh
stop-all.sh
å„èŠ‚ç‚¹è¿è¡Œè„šæœ¬åˆ°æ­¤ç¼–å†™å®Œæˆã€‚

æœ€å
chmod +x run_master.sh
chmod +x run_slave1.sh
chmod +x run_slave2.sh
chmod +x stop_master.sh
é€€å‡º Docker, å¹¶ä¿å­˜å‰¯æœ¬
$ exit
ä¿å­˜å‰¯æœ¬

$ docker commit -m "zookeeper hadoop spark scala install" 266b46cce542 ubuntu:spark
é…ç½®è™šæ‹Ÿæœº ubuntu çš„ hosts
$ vim /etc/hosts
æ·»åŠ å¦‚ä¸‹hosts

172.17.0.1      host
172.17.0.2      master
172.17.0.3      slave1
172.17.0.4      slave2
åˆ°æ­¤æ‰€æœ‰é…ç½®å®‰è£…åŸºæœ¬å®Œæˆäº†ï¼Œä¸‹é¢å¼€å¯ä½ çš„Sparké›†ç¾¤å§ï¼ï¼ï¼

å¯åŠ¨ Spark é›†ç¾¤
å¯åŠ¨ Master èŠ‚ç‚¹
$ docker run -ti -h master ubuntu:spark 
åœ¨è¿™é‡Œå…ˆä¸è¦ç€æ€¥ç€è¿è¡Œ run_master.sh å¯åŠ¨è„šæœ¬ã€‚ç­‰æœ€åå†è¿è¡Œ

å¯åŠ¨ Slave1 èŠ‚ç‚¹
$ docker run -ti -h slave1 ubuntu:spark 
è¿è¡Œ run_slave1.sh å¯åŠ¨è„šæœ¬

$ ./root/soft/shell/run_slave1.sh
å¯åŠ¨ Slave2 èŠ‚ç‚¹
$ docker run -ti -h slave2 ubuntu:spark 
è¿è¡Œ run_slave2.sh å¯åŠ¨è„šæœ¬

$ ./root/soft/shell/run_slave2.sh
æœ€åå†è¿è¡Œ Master èŠ‚ç‚¹çš„å¯åŠ¨è„šæœ¬ run_master.sh
åˆ‡æ¢åˆ°å¯åŠ¨äº† Master èŠ‚ç‚¹çš„ Docker ç»ˆç«¯

$ ./root/soft/shell/run_master.sh
å¯ä»¥ä½¿ç”¨ jps å‘½ä»¤æŸ¥çœ‹å½“å‰é›†ç¾¤è¿è¡Œæƒ…å†µ

$ jps
ä¸å‡ºæ„å¤–çš„è¯ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼å¦‚ä¸‹ä¿¡æ¯ï¼š

2081 QuorumPeerMain
3011 NodeManager
2900 ResourceManager
2165 JournalNode
2405 NameNode
3159 Worker
2503 DataNode
3207 Jps
åˆ°æ­¤å·²ç»å¯åŠ¨äº†ä½ çš„ Spark é›†ç¾¤äº†ã€‚

```

**è¿˜å¯ä»¥ç™»å½•webç®¡ç†å°æ¥æŸ¥çœ‹è¿è¡ŒçŠ¶å†µï¼š**
  æœåŠ¡        åœ°å€
 HDFS    master:50070
 Yarn      master:8088
 Spark    master:8080





## ç¯å¢ƒå˜é‡

```scala
# Setting PATH for Python 3.7
# The original version is saved in .bash_profile.pysave
PATH="/Library/Frameworks/Python.framework/Versions/3.7/bin:${PATH}"
export PATH

##env
JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home"
PATH="/usr/local/mysql/bin:$JAVA_HOME/bin:$PATH"
CLASSPATH=".:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar"
export JAVA_HOME PATH CLASSPATH

#go
export GOPATH=/Users/clude/Documents/gostudy/go_learning
export GOBIN=$GOPATH/bin
export PATH=$PATH:$GOBIN

# c#
export PATH=/Users/tianzi/Desktop/æ—¶é—´å¤æ‚åº¦/c\#/myApp:$PATH
export PATH=${PATH}:/usr/local/mysql/bin



#hadoop
export HADOOP_HOME=/usr/local/Cellar/hadoop/3.1.2/libexec
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin



# tree
alias tree="find . -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g'"

#mysql
alias mysql=/usr/local/mysql/bin/mysql


# added by Anaconda3 2019.07 installer
# >>> conda init >>>
# !! Contents within this block are managed by 'conda init' !!
__conda_setup="$(CONDA_REPORT_ERRORS=false '/Users/tianzi/anaconda3/bin/conda' shell.bash hook 2> /dev/null)"
if [ $? -eq 0 ]; then
    \eval "$__conda_setup"
else
    if [ -f "/Users/tianzi/anaconda3/etc/profile.d/conda.sh" ]; then
        . "/Users/tianzi/anaconda3/etc/profile.d/conda.sh"
        CONDA_CHANGEPS1=false conda activate base
    else
        \export PATH="/Users/tianzi/anaconda3/bin:$PATH"
    fi
fi
unset __conda_setup
# <<< conda init <<<


```



## sparkç¬”è®°



Spark æ ¸å¿ƒå’Œæ¡†æ¶æ„å»ºåœ¨ Scala( æ–¯å¡æ‹‰) ä¸Šï¼Œå¯¹åº”ç”¨å¼€å‘è€…ä¹Ÿæ˜¯è¨€å¿…ç§° Scalaï¼Œç›´æ¥ç§€èµ·äº†ä»£ç æç®€çš„è‚Œè‚‰ã€‚åœ¨ç”¨æˆ·æ¥å£ä¸Šï¼Œä»ä¼ä¸šçº§åº”ç”¨çš„é¦–é€‰ Javaï¼Œåˆ°æ•°æ®ç§‘å­¦å®¶çš„ Python å’Œ Rï¼Œå†åˆ°å•†ä¸šæ™ºèƒ½ BI çš„ SQLï¼Œå®˜æ–¹éƒ½ä¸€ä¸€æ”¯æŒ;

ç”±äºspark-shellåªæ”¯æŒscalaå’Œpythonä¸¤ç§è¯­è¨€çš„ç¼–å†™ï¼Œä¸æ”¯æŒJavaï¼Œæ‰€ä»¥æˆ‘åœ¨spark-shellä¸­é€šè¿‡scalaçš„è¯­æ³•æ¥è¿›è¡Œç®€å•æµ‹è¯•ã€‚

```
`scala> val textFile = sc.textFile("file:///usr/local/spark/README.md")>textFile: org.apache.spark.rdd.RDD[String] = file:///usr/local/spark/README.md MapPartitionsRDD[1] at textFile at <console>:24`
```

 

ä»£ç ä¸­é€šè¿‡ `file://` å‰ç¼€æˆ–è€…ä¸åŠ  `file://` å‰ç¼€è¡¨ç¤ºæŒ‡å®šè¯»å–æœ¬åœ°æ–‡ä»¶ã€‚



**Sparkçš„ç‰¹ç‚¹**
**1ï¼‰Speedï¼šå¿«é€Ÿï¼Œé«˜æ•ˆ**
Spark å…è®¸å°†ä¸­é—´è¾“å‡ºå’Œç»“æœå­˜å‚¨åœ¨å†…å­˜ä¸­ï¼ŒèŠ‚çœäº†å¤§é‡çš„ç£ç›˜ IO
ä½¿ç”¨æœ€å…ˆè¿›çš„ DAG è°ƒåº¦ç¨‹åºæŸ¥è¯¢ä¼˜åŒ–ç¨‹åºå’Œç‰©ç†æ‰§è¡Œå¼•æ“ï¼Œå®ç°æ‰¹é‡å’Œæµå¼æ•°æ®çš„é«˜æ€§èƒ½ã€‚
åŒæ—¶ Spark è‡ªèº«çš„ DAG æ‰§è¡Œå¼•æ“ä¹Ÿæ”¯æŒæ•°æ®åœ¨å†…å­˜ä¸­çš„è®¡ç®—ã€‚Spark å®˜ç½‘å£°ç§°æ€§èƒ½æ¯”
Hadoop å¿« 100 å€ã€‚å³ä¾¿æ˜¯å†…å­˜ä¸è¶³éœ€è¦ç£ç›˜ IOï¼Œå…¶é€Ÿåº¦ä¹Ÿæ˜¯ Hadoop çš„ 10 å€ä»¥ä¸Šã€‚

**2ï¼‰Easy for useï¼šç®€å•æ˜“ç”¨**
Spark ç°åœ¨æ”¯æŒ Javaã€Scalaã€Python å’Œ R ç­‰ç¼–ç¨‹è¯­è¨€ç¼–å†™åº”ç”¨ç¨‹åºï¼Œå¤§å¤§é™ä½äº†ä½¿ç”¨è€…çš„é—¨
æ§›ã€‚è‡ªå¸¦äº† 80 å¤šä¸ªç®—å­ã€‚å…è®¸åœ¨ Scalaï¼ŒPythonï¼ŒR çš„ shell ä¸­è¿›è¡Œäº¤äº’å¼æŸ¥è¯¢ï¼Œå¯
ä»¥éå¸¸æ–¹ä¾¿çš„åœ¨è¿™äº› Shell ä¸­ä½¿ç”¨ Spark é›†ç¾¤æ¥éªŒè¯è§£å†³é—®é¢˜çš„æ–¹æ³•ã€‚

**3ï¼‰Generalityï¼šå…¨æ ˆå¼æ•°æ®å¤„ç†**
Spark æä¾›äº†ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚Spark ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆéå¸¸å…·æœ‰å¸å¼•åŠ›ï¼Œæ¯•ç«Ÿä»»ä½•å…¬å¸éƒ½æƒ³ç”¨
ç»Ÿä¸€çš„å¹³å°å»å¤„ç†é‡åˆ°çš„é—®é¢˜ï¼Œå‡å°‘å¼€å‘å’Œç»´æŠ¤çš„äººåŠ›æˆæœ¬å’Œéƒ¨ç½²å¹³å°çš„ç‰©åŠ›æˆæœ¬ã€‚
æ”¯æŒæ‰¹å¤„ç†ï¼ˆSpark Coreï¼‰ã€‚Spark Core æ˜¯ Spark çš„æ ¸å¿ƒåŠŸèƒ½å®ç°ï¼ŒåŒ…æ‹¬ï¼šSparkContext çš„åˆå§‹
åŒ–ï¼ˆDriverApplication é€šè¿‡ SparkContext æäº¤ï¼‰ã€éƒ¨ç½²æ¨¡å¼ã€å­˜å‚¨ä½“ç³»ã€ä»»åŠ¡æäº¤ä¸æ‰§è¡Œã€è®¡
ç®—å¼•æ“ç­‰ã€‚

Spark SQLï¼šæ”¯æŒäº¤äº’å¼æŸ¥è¯¢ã€‚Spark SQL æ˜¯ Spark æ¥æ“ä½œç»“æ„åŒ–æ•°æ®çš„ç¨‹åºåŒ…ï¼Œå¯ä»¥è®©æˆ‘ä»¬
ä½¿ç”¨ SQL è¯­å¥çš„æ–¹å¼æ¥æŸ¥è¯¢æ•°æ®ï¼ŒSpark æ”¯æŒå¤šç§æ•°æ®æºï¼ŒåŒ…å« Hive è¡¨ï¼Œparquet ä»¥åŠ JSON
ç­‰å†…å®¹ã€‚

Spark Streamingï¼šæ”¯æŒæµå¼è®¡ç®—ã€‚ä¸ MapReduce åªèƒ½å¤„ç†ç¦»çº¿æ•°æ®ç›¸æ¯”ï¼ŒSpark è¿˜æ”¯æŒå®
æ—¶çš„æµè®¡ç®—ã€‚Spark ä¾èµ– Spark Streaming å¯¹æ•°æ®è¿›è¡Œå®æ—¶çš„å¤„ç†ã€‚

Spark MLlibï¼šæ”¯æŒæœºå™¨å­¦ä¹ ã€‚æä¾›æœºå™¨å­¦ä¹ ç›¸å…³çš„ç»Ÿè®¡ã€åˆ†ç±»ã€å›å½’ç­‰é¢†åŸŸçš„å¤šç§ç®—æ³•å®
ç°ã€‚å…¶ä¸€è‡´çš„ API æ¥å£å¤§å¤§é™ä½äº†ç”¨æˆ·çš„å­¦ä¹ æˆæœ¬ã€‚

Spark GraghXï¼šæ”¯æŒå›¾è®¡ç®—ã€‚æä¾›å›¾è®¡ç®—å¤„ç†èƒ½åŠ›ï¼Œæ”¯æŒåˆ†å¸ƒå¼ï¼ŒPregel æä¾›çš„ API å¯ä»¥
è§£å†³å›¾è®¡ç®—ä¸­çš„å¸¸è§é—®é¢˜ã€‚

PySparkï¼šæ”¯æŒ Python æ“ä½œ

SparkRï¼šæ”¯æŒ R è¯­è¨€æ“ä½œ


**4ï¼‰Runs Everywhereï¼šå…¼å®¹**

å¯ç”¨æ€§é«˜ï¼šSpark ä¹Ÿå¯ä»¥ä¸ä¾èµ–äºç¬¬ä¸‰æ–¹çš„èµ„æºç®¡ç†å’Œè°ƒåº¦å™¨ï¼Œå®ƒå®ç°äº† Standalone ä½œä¸ºå…¶
å†…ç½®çš„èµ„æºç®¡ç†å’Œè°ƒåº¦æ¡†æ¶ï¼Œè¿™æ ·è¿›ä¸€æ­¥é™ä½äº† Spark çš„ä½¿ç”¨é—¨æ§›ï¼Œä½¿å¾—æ‰€æœ‰äººéƒ½å¯ä»¥éå¸¸
å®¹æ˜“åœ°éƒ¨ç½²å’Œä½¿ç”¨ Sparkï¼Œæ­¤æ¨¡å¼ä¸‹çš„ Master å¯ä»¥æœ‰å¤šä¸ªï¼Œè§£å†³äº†å•ç‚¹æ•…éšœé—®é¢˜ã€‚å½“ç„¶ï¼Œæ­¤
æ¨¡å¼ä¹Ÿå®Œå…¨å¯ä»¥ä½¿ç”¨å…¶ä»–é›†ç¾¤ç®¡ç†å™¨æ›¿æ¢ï¼Œæ¯”å¦‚ YARNã€Mesosã€Kubernetesã€EC2 ç­‰ã€‚

ä¸°å¯Œçš„æ•°æ®æºæ”¯æŒï¼šSpark é™¤äº†å¯ä»¥è®¿é—®æ“ä½œç³»ç»Ÿè‡ªèº«çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå’Œ HDFS ä¹‹å¤–ï¼Œè¿˜å¯
ä»¥è®¿é—® Cassandraã€HBaseã€Hiveã€Tachyon ä»¥åŠä»»ä½• Hadoop çš„æ•°æ®æºã€‚

Spark æ”¯æŒçš„å‡ ç§éƒ¨ç½²æ–¹æ¡ˆï¼š
Mesosï¼šSpark å¯ä»¥è¿è¡Œåœ¨ Mesos é‡Œé¢ï¼ˆMesos ç±»ä¼¼äº YARN çš„ä¸€ä¸ªèµ„æºè°ƒåº¦æ¡†æ¶ï¼‰
Standaloneï¼šSpark è‡ªå·±å¯ä»¥ç»™è‡ªå·±åˆ†é…èµ„æºï¼ˆMasterï¼ŒWorkerï¼‰
YARNï¼šSpark å¯ä»¥è¿è¡Œåœ¨ Hadoop çš„ YARN ä¸Šé¢
Kubernetesï¼šSpark æ¥æ”¶ Kubernetes çš„èµ„æºè°ƒåº¦

**Spark Coreæä¾›Sparkæœ€åŸºç¡€æœ€æ ¸å¿ƒçš„åŠŸèƒ½ï¼š**

```
Spark Coreæä¾›Sparkæœ€åŸºç¡€æœ€æ ¸å¿ƒçš„åŠŸèƒ½ï¼š
ã€€ã€€SparkContextï¼š
ã€€ã€€ã€€ã€€ç¨‹åºçš„å…¥å£ã€‚
ã€€ã€€ã€€ã€€é€šå¸¸è€Œè¨€ï¼ŒDriverApplication çš„æ‰§è¡Œä¸è¾“å‡ºéƒ½æ˜¯é€šè¿‡ SparkContext æ¥å®Œæˆçš„ã€‚
ã€€ã€€ã€€ã€€DAGScheduler : å°†ä»»åŠ¡çš„é˜¶æ®µåˆ’åˆ†æˆä¸€ä¸ªä¸ªçš„Stageï¼ˆé‡åˆ°éœ€è¦Shuffleå°±åˆ‡åˆ†æˆä¸€ä¸ªStageï¼‰  ç„¶åå‘é€ç»™TaskScheduler
ã€€ã€€ã€€ã€€TaskScheduler : æ”¶åˆ°DAGSchedulerå‘é€è¿‡æ¥çš„Stageå˜æˆ  List<Task> äº¤ç»™èµ„æºè°ƒåº¦ç³»ç»Ÿå»è¿è¡Œ
ã€€ã€€å­˜å‚¨ä½“ç³»ï¼š
ã€€ã€€ã€€ã€€Sparkä¼˜å…ˆè€ƒè™‘ä½¿ç”¨å„èŠ‚ç‚¹çš„å†…å­˜ä½œä¸ºå­˜å‚¨ï¼Œå½“å†…å­˜ä¸è¶³æ—¶æ‰ä¼šè€ƒè™‘ä½¿ç”¨ç£ç›˜å­˜å‚¨ã€‚

ã€€ã€€è®¡ç®—å¼•æ“ ï¼š
ã€€ã€€ã€€ã€€ç”±äºå•èŠ‚ç‚¹ä¸è¶³ä»¥æä¾›è¶³å¤Ÿçš„å­˜å‚¨åŠè®¡ç®—èƒ½åŠ›ï¼Œ
ã€€ã€€ã€€ã€€æ‰€ä»¥ä½œä¸ºå¤§æ•°æ®å¤„ç†çš„ Spark åœ¨ SparkContext çš„ TaskScheduler ç»„ä»¶ä¸­æä¾›äº†
ã€€ã€€ã€€ã€€å¯¹ Standalone éƒ¨ç½²æ¨¡å¼çš„å®ç°å’Œ YARNã€Mesos ç­‰åˆ†å¸ƒå¼èµ„æºç®¡ç†ç³»ç»Ÿçš„æ”¯æŒã€‚

```

3ã€æ ¸å¿ƒæ¦‚å¿µ
ã€€ã€€ClusterManagerï¼š
ã€€ã€€ã€€ã€€Spark çš„é›†ç¾¤ç®¡ç†å™¨ï¼Œä¸»è¦è´Ÿè´£èµ„æºçš„åˆ†é…ä¸ç®¡ç†ã€‚
ã€€ã€€ã€€ã€€ç›®å‰ï¼ŒStandaloneã€YARNã€Mesosã€K8Sï¼ŒEC2 ç­‰éƒ½å¯ä»¥ä½œä¸º Spark
ã€€ã€€ã€€ã€€çš„é›†ç¾¤ç®¡ç†å™¨
ã€€ã€€Masterï¼š
ã€€ã€€ã€€ã€€é›†ç¾¤çš„ä¸»èŠ‚ç‚¹
ã€€ã€€Workerï¼š
ã€€ã€€ã€€ã€€ä¼šå¯åŠ¨å¾ˆå¤šä»»åŠ¡ï¼ˆExecutorï¼‰
ã€€ã€€Executorï¼š
ã€€ã€€ã€€ã€€æ‰§è¡Œè®¡ç®—ä»»åŠ¡çš„ä¸€äº›è¿›ç¨‹ã€‚
ã€€ã€€Driver Appicationï¼š
ã€€ã€€ã€€ã€€å®¢æˆ·ç«¯é©±åŠ¨ç¨‹åºï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºå®¢æˆ·ç«¯åº”ç”¨ç¨‹åºã€‚
ã€€ã€€ã€€ã€€ä¸€ä¸ªAppicationæœ‰å¤šä¸ªJob
ã€€ã€€Driverï¼š
ã€€ã€€ã€€ã€€SparkContext åœ¨ Driveré‡Œé¢ï¼ŒDAGSchedulerå’ŒtaskScheduleråœ¨SparkContexté‡Œé¢
ã€€ã€€Taskï¼š
ã€€ã€€ã€€ã€€ä¸€ä¸ªåˆ†åŒºæœ‰ä¸€ä¸ªTask
ã€€ã€€Stageï¼š
ã€€ã€€ã€€ã€€é‡åˆ°æœ‰shuffleçš„å°±åˆ’åˆ†æˆä¸€ä¸ªStage
ã€€ã€€ã€€ã€€ä¸€ä¸ªStageæœ‰å¤šä¸ªTask
ã€€ã€€Transformï¼š
ã€€ã€€ã€€ã€€ç”±ä¸€ä¸ªRDDè½¬æ¢æˆå¦ä¸€ä¸ªRDD
ã€€ã€€Actionï¼š
ã€€ã€€ã€€ã€€ç”±ADDè½¬æ¢æˆä¸€ä¸ªScalaçš„æ•°æ®ç±»å‹
ã€€ã€€Jobï¼š
ã€€ã€€ã€€ã€€ä¸€ä¸ªJobæœ‰å¤šä¸ªStage
ã€€ã€€ã€€ã€€é‡åˆ°ä¸€ä¸ªactionå°±å½¢æˆä¸€ä¸ªJob
å°ç»“ï¼š  application  ===ã€‹ Job  ===ã€‹Stage  ===ã€‹Task
ã€€ã€€executorï¼š
ã€€ã€€ã€€ã€€åŒ…å«å¤šä¸ªtask
ã€€ã€€ã€€ã€€ä¸€ä¸ªJVMè¿›ç¨‹ä»£è¡¨ä¸€ä¸ªexecutor
RDDï¼š
ã€€ã€€RDDçš„5å¤§ç‰¹æ€§
ã€€ã€€æºç ä¸­æœ‰è¿™æ ·çš„æ³¨é‡Š



```
* Internally, each RDD is characterized by five main properties:
*
*  - A list of partitions
*  - A function for computing each split
*  - A list of dependencies on other RDDs
*  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
*  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
*    an HDFS file)


```

1.a list of partiotioneræœ‰å¾ˆå¤šä¸ªpartiotioner(è¿™é‡Œæœ‰3ä¸ªpartiotioner),å¯ä»¥æ˜ç¡®çš„è¯´ï¼Œä¸€ä¸ªåˆ†åŒºåœ¨ä¸€å°æœºå™¨ä¸Šï¼Œä¸€ä¸ªåˆ†åŒºå…¶å®å°±æ˜¯æ”¾åœ¨ä¸€å°æœºå™¨çš„å†…å­˜ä¸Šï¼Œä¸€å°æœºå™¨ä¸Šå¯ä»¥æœ‰å¤šä¸ªåˆ†åŒºã€‚

ã€€ã€€2.a function for partiotionerä¸€ä¸ªå‡½æ•°ä½œç”¨åœ¨ä¸€ä¸ªåˆ†åŒºä¸Šã€‚æ¯”å¦‚è¯´ä¸€ä¸ªåˆ†åŒºæœ‰1ï¼Œ2,3 åœ¨rdd1.map(_*10),æŠŠRDDé‡Œé¢çš„æ¯ä¸€ä¸ªå…ƒç´ å–å‡ºæ¥ä¹˜ä»¥10ï¼Œæ¯ä¸ªåˆ†ç‰‡éƒ½åº”ç”¨è¿™ä¸ªmapçš„å‡½æ•°ã€‚

ã€€ã€€3.RDDä¹‹é—´æœ‰ä¸€ç³»åˆ—çš„ä¾èµ–rdd1.map(_*10).flatMap(..).map(..).reduceByKey(...)ï¼Œæ„å»ºæˆä¸ºDAG,è¿™ä¸ªDAGä¼šæ„é€ æˆå¾ˆå¤šä¸ªé˜¶æ®µï¼Œè¿™äº›é˜¶æ®µå«åšstageï¼ŒRDDstageä¹‹é—´ä¼šæœ‰ä¾èµ–å…³ç³»ï¼Œåé¢æ ¹æ®å‰é¢çš„ä¾èµ–å…³ç³»æ¥æ„å»ºï¼Œå¦‚æœå‰é¢çš„æ•°æ®ä¸¢äº†ï¼Œå®ƒä¼šè®°ä½å‰é¢çš„ä¾èµ–ï¼Œä»å‰é¢è¿›è¡Œé‡æ–°æ¢å¤ã€‚æ¯ä¸€ä¸ªç®—å­éƒ½ä¼šäº§ç”Ÿæ–°çš„RDDã€‚textFile ä¸flatMapä¼šäº§ç”Ÿä¸¤ä¸ªRDD.

ã€€ã€€4.åˆ†åŒºå™¨hash & Integer.Max % partiotioner å†³å®šæ•°æ®åˆ°å“ªä¸ªåˆ†åŒºé‡Œé¢ï¼Œå¯é€‰ï¼Œè¿™ä¸ªRDDæ˜¯key-value çš„æ—¶å€™æ‰èƒ½æœ‰

ã€€ã€€5.æœ€ä½³ä½ç½®ã€‚æ•°æ®åœ¨å“ªå°æœºå™¨ä¸Šï¼Œä»»åŠ¡å°±å¯åœ¨å“ªä¸ªæœºå™¨ä¸Šï¼Œæ•°æ®åœ¨æœ¬åœ°ä¸Šï¼Œä¸ç”¨èµ°ç½‘ç»œã€‚ä¸è¿‡æ•°æ®è¿›è¡Œæœ€åæ±‡æ€»çš„æ—¶å€™å°±è¦èµ°ç½‘ç»œã€‚ï¼ˆhdfs fileçš„blockå—ï¼‰

 

ç®—å­ï¼š
Transformations
ã€€ã€€1ã€groupByKey(numPartation)
ã€€ã€€ã€€ã€€å¯ä»¥ä¼ å…¥ä¸€ä¸ªåˆ†åŒºä¸ªæ•°çš„å‚æ•°
ã€€ã€€2ã€reduceByKey(func,[numPartation])
ã€€ã€€ã€€ã€€æœ‰shuffle
ã€€ã€€3ã€aggregateByKey(func)
ã€€ã€€ã€€ã€€å±€éƒ¨èšåˆï¼Œæ²¡æœ‰shuffleï¼Œç›¸å½“äºä¸€ä¸ªmapReduce åªæœ‰combiner æ²¡æœ‰reducer
ã€€ã€€4ã€flatten()
ã€€ã€€ã€€ã€€å‹å¹³
ã€€ã€€5ã€union(otherDataSet)
ã€€ã€€ã€€ã€€å°†ä¸¤ä¸ªrddçš„æ•°æ®ç»“åˆæˆä¸€ä¸ªrdd
ã€€ã€€6ã€cogroup(otherDataset, [numPartitions])
ã€€ã€€ã€€ã€€arr1 : (a,1),(a,2),(b,1),(c,1)
ã€€ã€€ã€€ã€€arr2 :(a,3),(a,4),(b,5),(c,6)
ã€€ã€€ã€€arr1.cogroup(arr2) = (  a,   (   (1,2), (3,4)   )   ) ,    (    b,  (  (1),(5)  )  ) , ( c, (  (1),(6)  )  ) 
 ã€€ã€€7ã€cartesian(otherDataset)ã€€ç¬›å¡å°”ç§¯
ã€€ã€€8ã€pipe(command, [envVars])
ã€€ã€€ã€€ã€€æ‰§è¡Œä¸€ä¸ªå‘½ä»¤ã€‚ ä¾‹å¦‚æ‰§è¡Œä¸€ä¸ªpythonã€shellè„šæœ¬
ã€€ã€€9ã€coalesce(numPartitions)
ã€€ã€€ã€€ã€€é‡æ–°åˆ†åŒºï¼Œå‡å°‘æˆ–è€…å¢åŠ åˆ†åŒº
ã€€ã€€10ã€repartition(numPartitions)
ã€€ã€€ã€€ã€€é‡æ–°åˆ†åŒº å¯èƒ½ä¼šè§¦å‘shuffleï¼Œå°½é‡é¿å…ä½¿ç”¨
ã€€ã€€11ã€repartitionAndSortWithinPartitions(partitioner)
ã€€ã€€ã€€ã€€é‡æ–°åˆ†åŒºå¹¶ä¸”æŒ‰ç…§åˆ†åŒºå†…æ’åº

**Action**
ã€€ã€€1ã€reduce(func)
ã€€ã€€2ã€collect()
ã€€ã€€3ã€count()
ã€€ã€€4ã€take(n)
ã€€ã€€5ã€takeSample(withReplacement, num, [seed])
ã€€ã€€6ã€countByKey()
ã€€ã€€7ã€foreach(func)

ã€€ã€€

**Sparkæ‰§è¡Œæµç¨‹ï¼š**

1ã€SparkContextåˆå§‹åŒ–
1 ã€å…ˆåˆå§‹åŒ–ä¸€ä¸ªSparkConf
2ã€ åˆ›å»ºä¸€ä¸ªTaskScheduler
3ã€ åˆ›å»ºä¸€ä¸ªDagScheduler
DagScheduler çš„ä½œç”¨æ˜¯å°†RDDä¾èµ–åˆ‡åˆ†æˆä¸€ä¸ªä¸ªStageï¼Œç„¶åå°†Stageè½¬æˆTaskSetæäº¤ç»™DriverActor
4ã€åœ¨åˆ›å»ºTaskSchedulerçš„åŒæ—¶ä¹Ÿä¼šåˆ›å»ºä¸¤ä¸ªå¾ˆé‡è¦çš„å¯¹è±¡ï¼š DriverActor å’Œ ClientActor
ClientActorçš„ä½œç”¨æ˜¯å‘Masteræ³¨å†Œç”¨æˆ·æäº¤çš„ä»»åŠ¡
DriverActorçš„ä½œç”¨æ˜¯æ¥å—Executorçš„åå‘æ³¨å†Œï¼Œ å°†ä»»åŠ¡æäº¤åˆ°Executor

5ã€å½“ClientActor å¯åŠ¨æˆåŠŸçš„æ—¶å€™ï¼Œå®ƒä¼šå°†ç”¨æˆ·æäº¤çš„ä»»åŠ¡å’Œç›¸å…³çš„å‚æ•°å°è£…åˆ° ApplicationDescription å¯¹è±¡ä¸­ï¼Œç„¶åæäº¤ç»™ Master  è¿›è¡Œä»»åŠ¡çš„æ³¨å†Œ
6ã€å½“Masteræ¥æ”¶åˆ°ClientActorçš„ä»»åŠ¡æ³¨å†Œè¯·æ±‚çš„æ—¶å€™ï¼Œä¼šå°†è¯·æ±‚å‚æ•°è¿›è¡Œè§£æï¼Œå¹¶å°è£…æˆApplicationï¼Œç„¶åå°†å®ƒæŒä¹…åŒ–ä»»åŠ¡é˜Ÿåˆ— waitingAppsä¸­
7ã€å½“è½®åˆ°æˆ‘ä»¬æäº¤çš„ä»»åŠ¡è¿è¡Œæ—¶ï¼Œå°±å¼€å§‹è°ƒç”¨ schedule()ï¼Œè¿›è¡Œä»»åŠ¡èµ„æºçš„è°ƒåº¦
8ã€Master å°†è°ƒåº¦å¥½çš„èµ„æºå°è£…åˆ° launchExecutor ä¸­å‘é€ç»™æŒ‡å®šçš„ Worker
9ã€Workeræ¥æ”¶åˆ°Masterå‘è¿‡æ¥çš„ launchExecutor æ—¶ï¼Œä¼šå°†å®ƒè§£å‹å¹¶å°è£…åˆ° ExecutorRunnerä¸­ï¼Œç„¶åè°ƒç”¨è¿™ä¸ªå¯¹è±¡çš„start()æ–¹æ³•ï¼Œå¯åŠ¨Executorã€‚
10ã€Executor å¯åŠ¨åä¼šå‘DriverActorè¿›è¡Œåå‘æ³¨å†Œ
11ã€DriverActorè¿”å›ä¸€ä¸ªæ³¨å†ŒæˆåŠŸçš„æ¶ˆæ¯ä¸ªExecutor
12ã€Executoræ¥æ”¶åˆ°æ³¨å†ŒæˆåŠŸçš„æ¶ˆæ¯ååˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ± ï¼Œç”¨äºæ‰§è¡ŒDriverActorå‘é€è¿‡æ¥çš„å‘é€è¿‡æ¥çš„taskä»»åŠ¡
13ã€å½“å±äºè¿™ä¸ªä»»åŠ¡çš„æ‰€æœ‰çš„ Executor å¯åŠ¨å¹¶åå‘æ³¨å†ŒæˆåŠŸåï¼Œå°±æ„å‘³ç€è¿è¡Œè¿™ä¸ªä»»åŠ¡çš„ç¯å¢ƒå·²ç»å‡†å¤‡å¥½äº†ï¼Œdriver ä¼šç»“æŸ SparkContext å¯¹è±¡çš„åˆå§‹åŒ–ï¼Œä¹Ÿå°±æ„å‘³ç€ new SparkContextè¿™å¥ä»£ç è¿è¡Œå®Œæˆ
14ã€å½“åˆå§‹åŒ– sc æˆåŠŸåï¼Œdriver ç«¯å°±ä¼šç»§ç»­è¿è¡Œæˆ‘ä»¬ç¼–å†™çš„ä»£ç ï¼Œç„¶åå¼€å§‹åˆ›å»ºåˆå§‹çš„ RDDï¼Œç„¶åè¿›è¡Œä¸€ç³»åˆ—è½¬æ¢æ“ä½œï¼Œå½“é‡åˆ°ä¸€ä¸ª action ç®—å­æ—¶ï¼Œä¹Ÿå°±æ„å‘³ç€è§¦å‘äº†ä¸€ä¸ª job

15ã€Driver ä¼šå°†è¿™ä¸ª job æäº¤ç»™ DAGScheduler
16ã€DAGScheduler å°†æ¥å—åˆ°çš„ jobï¼Œä»æœ€åä¸€ä¸ªç®—å­å‘å‰æ¨å¯¼ï¼Œå°† DAG ä¾æ®å®½ä¾èµ–åˆ’åˆ†æˆä¸€ä¸ªä¸€ä¸ªçš„ stageï¼Œç„¶åå°† stage å°è£…æˆ taskSetï¼Œå¹¶å°† taskSet ä¸­çš„ task æäº¤ç»™ DriverActor
17ã€DriverActor æ¥å—åˆ° DAGScheduler å‘é€è¿‡æ¥çš„ taskï¼Œä¼šæ‹¿åˆ°ä¸€ä¸ªåºåˆ—åŒ–å™¨ï¼Œå¯¹ task è¿›è¡Œåºåˆ—åŒ–ï¼Œç„¶åå°†åºåˆ—åŒ–å¥½çš„ task å°è£…åˆ° launchTask ä¸­ï¼Œç„¶åå°† launchTask å‘é€ç»™æŒ‡å®šçš„Executor
18ã€Executor æ¥å—åˆ°äº† DriverActor å‘é€è¿‡æ¥çš„ launchTask æ—¶ï¼Œä¼šæ‹¿åˆ°ä¸€ä¸ªååºåˆ—åŒ–å™¨ï¼Œå¯¹launchTask è¿›è¡Œååºåˆ—åŒ–ï¼Œå°è£…åˆ° TaskRunner ä¸­ï¼Œç„¶åä» Executor è¿™ä¸ªçº¿ç¨‹æ± ä¸­è·å–ä¸€ä¸ªçº¿ç¨‹ï¼Œå°†ååºåˆ—åŒ–å¥½çš„ä»»åŠ¡ä¸­çš„ç®—å­ä½œç”¨åœ¨ RDD å¯¹åº”çš„åˆ†åŒºä¸Š



**Spark  åœ¨ä¸åŒé›†ç¾¤ä¸­çš„æ¶æ„**ã€€ã€€

ã€€ã€€Standaloneæ¨¡å¼ï¼ˆSpark è‡ªå¸¦çš„æ¨¡å¼ï¼‰ã€YARN-Client æ¨¡å¼æˆ–è€… YARN-Cluster æ¨¡å¼

## å‚è€ƒèµ„æ–™

````
ä»0å¼€å§‹ä½¿ç”¨Dockeræ­å»ºSparké›†ç¾¤
https://www.jianshu.com/p/ee210190224f

Macä¸‹sparkç¯å¢ƒçš„æ­å»º
https://www.jianshu.com/p/31c7f6a5fc7e
````

